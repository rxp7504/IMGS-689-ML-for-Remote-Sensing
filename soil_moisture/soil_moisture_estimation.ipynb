{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FBmNtFE_gWU0"
      },
      "outputs": [],
      "source": [
        "!earthengine authenticate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import ee\n",
        "import geemap\n",
        "ee.Initialize()"
      ],
      "metadata": {
        "id": "fMJvINcZPgCj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#----------------- MAP VISUALIZER ---------------------\n",
        "scan_loc = [-77.93,40.72] # station coordinates (long,lat)\n",
        "site_name = \"Rock Springs, PA\"\n",
        "start_date = '2024-01-01'\n",
        "end_date = '2024-12-31'\n",
        "# SCAN site location\n",
        "point = ee.Geometry.Point(scan_loc)\n",
        "# Load SMAP\n",
        "smap_collection = ee.ImageCollection(\"NASA/SMAP/SPL3SMP_E/006\") \\\n",
        "  .filterDate(start_date, end_date) \\\n",
        "  .filterBounds(point) \\\n",
        "\n",
        "# Load one SMAP image from the collection\n",
        "sample_image = smap_collection.first()\n",
        "\n",
        "# Get the projection from the image\n",
        "projection = sample_image.select('soil_moisture_am').projection()\n",
        "\n",
        "# SMAP GSD\n",
        "pixel_bounds = point.buffer(4500).bounds()\n",
        "\n",
        "# PRISM grid\n",
        "prism_bounds = point.buffer(4638.3/2).bounds()\n",
        "\n",
        "# Visualize with geemap\n",
        "import geemap\n",
        "Map = geemap.Map()\n",
        "Map.center_object(point, 8)\n",
        "Map.addLayer(pixel_bounds, {'color': 'red'}, 'SMAP Approximation')\n",
        "Map.addLayer(prism_bounds, {'color': 'blue'}, 'PRISM Approximation')\n",
        "Map.addLayer(point, {'color': 'green'}, 'SCAN Site')\n",
        "Map\n"
      ],
      "metadata": {
        "id": "DfzgZUXYiehb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Extraction\n",
        " - Pull data from SCAN sites stored in local CSV files\n",
        " - Pull SMAP data from Google Earth Engine\n",
        " - Pull PRISM meterological data from Google Earth Engine"
      ],
      "metadata": {
        "id": "Ac9HTHWZPuVG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#--------------Map function for extracting soil moisture data---------------\n",
        "def extract_soil_moisture(image,point):\n",
        "\n",
        "    # Extract dates\n",
        "    date = ee.Date(image.date()).format('YYYY-MM-dd')\n",
        "\n",
        "    # Extract soil moisture value (PM)\n",
        "    sm_value = image.select('soil_moisture_pm').reduceRegion(\n",
        "        reducer=ee.Reducer.first(),\n",
        "        geometry=point,\n",
        "        scale=10000\n",
        "    ).get('soil_moisture_pm')\n",
        "\n",
        "    # Extract Weighted average of horizontally polarized brightness temperatures (PM)\n",
        "    bth_value = image.select('tb_h_corrected_pm').reduceRegion(\n",
        "        reducer=ee.Reducer.first(),\n",
        "        geometry=point,\n",
        "        scale=10000\n",
        "    ).get('tb_h_corrected_pm')\n",
        "\n",
        "    # Extract Weighted average of vertically polarized brightness temperatures (PM)\n",
        "    btv_value = image.select('tb_v_corrected_pm').reduceRegion(\n",
        "        reducer=ee.Reducer.first(),\n",
        "        geometry=point,\n",
        "        scale=10000\n",
        "    ).get('tb_v_corrected_pm')\n",
        "\n",
        "    # Extract quality flags (PM)\n",
        "    qual_flag_v = image.select('tb_qual_flag_v_pm').reduceRegion(\n",
        "        reducer=ee.Reducer.first(),\n",
        "        geometry=point,\n",
        "        scale=10000\n",
        "    ).get('tb_qual_flag_v_pm')\n",
        "\n",
        "    qual_flag_h = image.select('tb_qual_flag_h_pm').reduceRegion(\n",
        "        reducer=ee.Reducer.first(),\n",
        "        geometry=point,\n",
        "        scale=10000\n",
        "    ).get('tb_qual_flag_h_pm')\n",
        "\n",
        "    return image.set('date', date).set('sm_value', sm_value).set('bth_value', bth_value).set('btv_value', btv_value).set('qual_flag_v', qual_flag_v).set('qual_flag_h', qual_flag_h)\n",
        "\n",
        "\n",
        "# ----------------------- FUNCTION TO IMPORT SMAP DATA -------------------------\n",
        "def smap_extractor(scan_loc,start_date,end_date):\n",
        "  # SCAN site location\n",
        "  point = ee.Geometry.Point(scan_loc)\n",
        "\n",
        "  # Load SMAP\n",
        "  smap_collection = ee.ImageCollection(\"NASA/SMAP/SPL3SMP_E/006\") \\\n",
        "    .filterDate(start_date, end_date) \\\n",
        "    .filterBounds(point) \\\n",
        "\n",
        "  # Print number of images in the collection\n",
        "  print(f\"Number of images in the collection before filtering: {smap_collection.size().getInfo()}\")\n",
        "\n",
        "  # Map over the image collection and filter out None values\n",
        "  smap_features = smap_collection.map(lambda image: extract_soil_moisture(image, point)).filter(ee.Filter.notNull(['sm_value', 'qual_flag_v','qual_flag_h'])) # Pass 'point' to extract_soil_moisture\n",
        "\n",
        "  # Filter to only the samples that have good quality for brightness temperature\n",
        "  smap_features = smap_features.filter(\n",
        "      ee.Filter.And(\n",
        "          ee.Filter.eq('qual_flag_v', 0),\n",
        "          ee.Filter.eq('qual_flag_h', 0)\n",
        "      )\n",
        "  )\n",
        "\n",
        "  # Number of images in the collection after filtering\n",
        "  count = smap_features.size()\n",
        "  print(f\"Number of images in the collection after filtering: {count.getInfo()}\")\n",
        "\n",
        "  # Extract the time series data by aggregate data\n",
        "  sm_values = smap_features.aggregate_array('sm_value').getInfo()\n",
        "\n",
        "  # Extract corresponding dates\n",
        "  dates = smap_features.aggregate_array('date').getInfo()\n",
        "\n",
        "  # Extract brightness temperatures\n",
        "  bth_values = smap_features.aggregate_array('bth_value').getInfo()\n",
        "  btv_values = smap_features.aggregate_array('btv_value').getInfo()\n",
        "\n",
        "  # Print lengths to check if they match\n",
        "  print(f\"Length of SM values: {len(sm_values)}\")\n",
        "  print(f\"Length of Dates: {len(dates)}\")\n",
        "  print(f\"Length of Brightness Temperature (Horizontal): {len(bth_values)}\")\n",
        "  print(f\"Length of Brightness Temperature (Vertical): {len(btv_values)}\")\n",
        "\n",
        "  return dates, sm_values, bth_values, btv_values\n",
        "\n",
        "#--------------Map function for extracting prism weather data---------------\n",
        "def extract_weather_data(image,point):\n",
        "\n",
        "    # Extract dates\n",
        "    date = ee.Date(image.date()).format('YYYY-MM-dd')\n",
        "\n",
        "    # Extract Daily total precipitation [mm]\n",
        "    precip_value = image.select('ppt').reduceRegion(\n",
        "        reducer=ee.Reducer.first(),\n",
        "        geometry=point,\n",
        "        scale=5000\n",
        "    ).get('ppt')\n",
        "\n",
        "    # Extract daily mean temperature [C]\n",
        "    temp_value = image.select('tmean').reduceRegion(\n",
        "        reducer=ee.Reducer.first(),\n",
        "        geometry=point,\n",
        "        scale=5000\n",
        "    ).get('tmean')\n",
        "\n",
        "    # Extract Daily mean dew point temperature [C]\n",
        "    dew_value = image.select('tdmean').reduceRegion(\n",
        "        reducer=ee.Reducer.first(),\n",
        "        geometry=point,\n",
        "        scale=5000\n",
        "    ).get('tdmean')\n",
        "\n",
        "    # Extract Daily minimum vapor pressure deficit [hPa]\n",
        "    vpdmin_value = image.select('vpdmin').reduceRegion(\n",
        "        reducer=ee.Reducer.first(),\n",
        "        geometry=point,\n",
        "        scale=5000\n",
        "    ).get('vpdmin')\n",
        "\n",
        "    # Extract Daily maximum vapor pressure deficit [hPa]\n",
        "    vpdmax_value = image.select('vpdmax').reduceRegion(\n",
        "        reducer=ee.Reducer.first(),\n",
        "        geometry=point,\n",
        "        scale=5000\n",
        "    ).get('vpdmax')\n",
        "\n",
        "    return image.set('date', date).set('precip_value', precip_value).set('temp_value', temp_value).set('dew_value', dew_value).set('vpdmin_value', vpdmin_value).set('vpdmax_value', vpdmax_value)\n",
        "\n",
        "# ----------------------- FUNCTION TO IMPORT PRISM DATA -------------------------\n",
        "def prism_extractor(scan_loc,start_date,end_date):\n",
        "\n",
        "    # SCAN site location\n",
        "    point = ee.Geometry.Point(scan_loc)\n",
        "\n",
        "    prism_collection = ee.ImageCollection(\"OREGONSTATE/PRISM/AN81d\")\\\n",
        "        .filterDate(start_date, end_date) \\\n",
        "        .filterBounds(point)\n",
        "\n",
        "    # Print number of images in the collection\n",
        "    print(f\"Number of images in the collection: {prism_collection.size().getInfo()}\")\n",
        "\n",
        "    # Pull weather data from each image\n",
        "    prism_features = prism_collection.map(lambda image: extract_weather_data(image, point))\n",
        "\n",
        "    # Extract dates\n",
        "    dates = prism_features.aggregate_array('date').getInfo()\n",
        "\n",
        "    # Extract the time series data by aggregate data\n",
        "    precip_values = prism_features.aggregate_array('precip_value').getInfo()\n",
        "\n",
        "    # Extract mean daily temperature values\n",
        "    temp_values = prism_features.aggregate_array('temp_value').getInfo()\n",
        "\n",
        "    # Extract mean daily dewpoint values\n",
        "    dew_values = prism_features.aggregate_array('dew_value').getInfo()\n",
        "\n",
        "    # Extract vapor pressure deficit values\n",
        "    vpdmin_values = prism_features.aggregate_array('vpdmin_value').getInfo()\n",
        "    vpdmax_values = prism_features.aggregate_array('vpdmax_value').getInfo()\n",
        "\n",
        "    return dates, precip_values, temp_values, dew_values, vpdmin_values, vpdmax_values\n",
        "\n",
        "\n",
        "# ---------------- FUNCTION TO PLOT TIMESERIES AT LOCATION ---------------------\n",
        "\n",
        "def plot_all_time_series(merged_df, site_name):\n",
        "    # First figure: Soil Moisture\n",
        "    fig1, ax1 = plt.subplots(figsize=(12, 5))\n",
        "    plt.plot(merged_df['date'], merged_df['sm_smap'], label='SMAP')\n",
        "    plt.plot(merged_df['date'], merged_df['soil_moisture'], label='In-situ')\n",
        "    plt.xlabel('Date')\n",
        "    plt.ylabel('Soil Moisture [%]')\n",
        "    plt.title(f'Soil Moisture Time Series (SMAP vs In-situ): {site_name}')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Second figure: 2x2 grid for the rest\n",
        "    fig2, axs = plt.subplots(2, 2, figsize=(12, 5))\n",
        "\n",
        "    # Plot 1: Brightness Temperature\n",
        "    axs[0, 0].plot(merged_df['date'], merged_df['bth'], label='Horizontal')\n",
        "    axs[0, 0].plot(merged_df['date'], merged_df['btv'], label='Vertical')\n",
        "    axs[0, 0].set_ylabel('Brightness Temp [K]')\n",
        "    axs[0, 0].set_title(f'Brightness Temperature (SMAP)')\n",
        "    axs[0, 0].legend()\n",
        "    axs[0, 0].grid(True)\n",
        "\n",
        "    # Plot 2: Precipitation\n",
        "    axs[0, 1].plot(merged_df['date'], merged_df['precip'], color='blue')\n",
        "    axs[0, 1].set_ylabel('Precipitation [mm]')\n",
        "    axs[0, 1].set_title(f'Precipitation (PRISM)')\n",
        "    axs[0, 1].grid(True)\n",
        "\n",
        "    # Plot 3: Temperature and Dew Point\n",
        "    axs[1, 0].plot(merged_df['date'], merged_df['temp'], label='Temperature')\n",
        "    axs[1, 0].plot(merged_df['date'], merged_df['dew'], label='Dew Point')\n",
        "    axs[1, 0].set_ylabel('Temperature [°C]')\n",
        "    axs[1, 0].set_title(f'Temperature (PRISM)')\n",
        "    axs[1, 0].legend()\n",
        "    axs[1, 0].grid(True)\n",
        "\n",
        "    # Plot 4: Vapor Pressure Deficit\n",
        "    axs[1, 1].plot(merged_df['date'], merged_df['vpdmin'], label='VPD Minimum')\n",
        "    axs[1, 1].plot(merged_df['date'], merged_df['vpdmax'], label='VPD Maximum')\n",
        "    axs[1, 1].set_ylabel('VPD [hPa]')\n",
        "    axs[1, 1].set_title(f'Vapor Pressure Deficit (PRISM)')\n",
        "    axs[1, 1].legend()\n",
        "    axs[1, 1].grid(True)\n",
        "\n",
        "    # X-axis labels for bottom plots\n",
        "    for ax in axs[1, :]:\n",
        "        ax.set_xlabel('Date')\n",
        "\n",
        "    plt.suptitle(f'Time Series Data for {site_name}', fontsize=16)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# ------------------ FUNCTION TO BUILD ALL MOISTURE DATA ---------------------\n",
        "def build_moisture_data(scan_loc,site_name,start_date,end_date,scan_path):\n",
        "\n",
        "    # Import SCAN Data (ground truth)\n",
        "    sm_scan = pd.read_csv(scan_path)\n",
        "\n",
        "    # Convert 'date' column to datetime objects\n",
        "    sm_scan['date'] = pd.to_datetime(sm_scan['date'])\n",
        "\n",
        "    # Import SMAP data for [Date, Soil Moisture, Brightness Temp. Horizontal, Brightness Temp. Vertical] at SCAN coordinates\n",
        "    smap_dates, sm_smap, bth_values, btv_values = smap_extractor(scan_loc,start_date,end_date)\n",
        "\n",
        "    # Import PRISM data for [Date, Precipitation, Temperature, Dew Point, Vapor Pressure Min, Vapor Pressure Max] at SCAN coordinates\n",
        "    prism_dates, precip_values, temp_values, dew_values, vpdmin_values, vpdmax_values = prism_extractor(scan_loc,start_date,end_date)\n",
        "\n",
        "    # ------------ PLOT TIME SERIES -----------------\n",
        "\n",
        "    # Convert SMAP data to dataframe\n",
        "    smap_df = pd.DataFrame({\n",
        "        'date': smap_dates,\n",
        "        'sm_smap': [val * 100 for val in sm_smap],  # scale values by 100\n",
        "        'bth': bth_values,\n",
        "        'btv': btv_values\n",
        "    })\n",
        "    smap_df['date'] = pd.to_datetime(smap_df['date'])\n",
        "\n",
        "    # Convert PRISM data to dataframe\n",
        "    prism_df = pd.DataFrame({\n",
        "        'date': prism_dates,\n",
        "        'precip': precip_values,\n",
        "        'temp': temp_values,\n",
        "        'dew': dew_values,\n",
        "        'vpdmin': vpdmin_values,\n",
        "        'vpdmax': vpdmax_values\n",
        "    })\n",
        "    prism_df['date'] = pd.to_datetime(prism_df['date'])\n",
        "\n",
        "    # Merge SMAP and PRISM first\n",
        "    merged_df = pd.merge(sm_scan, smap_df, on='date', how='inner')\n",
        "\n",
        "    # Then merge the result with SMAP-SCAN\n",
        "    merged_df = pd.merge(merged_df, prism_df, on='date', how='inner')\n",
        "\n",
        "    # Check for empty values in SCAN data and remove\n",
        "    if merged_df.isnull().values.any() == True:\n",
        "       nullcount = merged_df.isnull().sum().sum()\n",
        "       print(f\"Found {nullcount} empty values in SCAN data. Removing corresponding rows...\")\n",
        "       merged_df = merged_df.dropna(subset=['soil_moisture'])\n",
        "\n",
        "\n",
        "    # Plot time series data\n",
        "    plot_all_time_series(merged_df, site_name)\n",
        "\n",
        "    # Save to csv\n",
        "    merged_df.to_csv('new_data.csv', index=False)\n",
        "\n",
        "    print(merged_df.head())\n",
        "    return merged_df"
      ],
      "metadata": {
        "id": "_w-kQrpbM9vF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------- IMPORT DATA FROM LOVELL SUMMIT, NV ---------------------\n",
        "\n",
        "# SCAN site location\n",
        "scan_loc = [-115.61,36.17] # station coordinates (long,lat)\n",
        "site_name = \"Lovell Summit, NV\"\n",
        "start_date = '2020-01-01'\n",
        "end_date = '2024-12-31'\n",
        "scan_path = '/content/drive/MyDrive/Imaging Science MS/Applied ML for Remote Sensing/Project/Data/SCAN Data/lovell_summit_20-25.csv'\n",
        "\n",
        "lovell_summit_df = build_moisture_data(scan_loc,site_name,start_date,end_date,scan_path)"
      ],
      "metadata": {
        "id": "46JgXwsfP5VN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------- IMPORT DATA FROM ROCK SPRINGS, PA ---------------------\n",
        "\n",
        "# SCAN site location\n",
        "scan_loc = [-77.93,40.72] # station coordinates (long,lat)\n",
        "site_name = \"Rock Springs, PA\"\n",
        "start_date = '2020-01-01'\n",
        "end_date = '2024-12-31'\n",
        "scan_path = '/content/drive/MyDrive/Imaging Science MS/Applied ML for Remote Sensing/Project/Data/SCAN Data/rock_springs_20-25.csv'\n",
        "\n",
        "rock_springs_df = build_moisture_data(scan_loc,site_name,start_date,end_date,scan_path)"
      ],
      "metadata": {
        "id": "YuQUeYgyVpUI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------- IMPORT DATA FROM Wakulla #1, FL ---------------------\n",
        "\n",
        "# SCAN site location\n",
        "scan_loc = [-84.42,30.31] # station coordinates (long,lat)\n",
        "site_name = \"Wakulla #1, FL\"\n",
        "start_date = '2020-01-01'\n",
        "end_date = '2024-12-31'\n",
        "scan_path = '/content/drive/MyDrive/Imaging Science MS/Applied ML for Remote Sensing/Project/Data/SCAN Data/wakulla_20-25.csv'\n",
        "\n",
        "wakulla_df = build_moisture_data(scan_loc,site_name,start_date,end_date,scan_path)"
      ],
      "metadata": {
        "id": "wf0fsQpxJOHv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------- IMPORT DATA FROM Dee River Ranch, AL ---------------------\n",
        "# SCAN site location\n",
        "scan_loc = [-84.42,30.31] # station coordinates (long,lat)\n",
        "site_name = \"Dee River Ranch, AL\"\n",
        "start_date = '2020-01-01'\n",
        "end_date = '2024-12-31'\n",
        "scan_path = '/content/drive/MyDrive/Imaging Science MS/Applied ML for Remote Sensing/Project/Data/SCAN Data/dee_river_ranch_20-25.csv'\n",
        "\n",
        "dee_river_df = build_moisture_data(scan_loc,site_name,start_date,end_date,scan_path)"
      ],
      "metadata": {
        "id": "2jUKAvBvcYBS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------- IMPORT DATA FROM Ames, IA ---------------------\n",
        "# SCAN site location\n",
        "scan_loc = [-93.73,\t42.02] # station coordinates (long,lat)\n",
        "site_name = \"Ames, IA\"\n",
        "start_date = '2020-01-01'\n",
        "end_date = '2024-12-31'\n",
        "scan_path = '/content/drive/MyDrive/Imaging Science MS/Applied ML for Remote Sensing/Project/Data/SCAN Data/ames_20-25.csv'\n",
        "\n",
        "ames_df = build_moisture_data(scan_loc,site_name,start_date,end_date,scan_path)"
      ],
      "metadata": {
        "id": "BsvYrXVJRH4x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------- IMPORT DATA FROM Lindsay, MT ---------------------\n",
        "# SCAN site location\n",
        "scan_loc = [-105.19,\t47.21] # station coordinates (long,lat)\n",
        "site_name = \"Lindsay, MT\"\n",
        "start_date = '2020-01-01'\n",
        "end_date = '2024-12-31'\n",
        "scan_path = '/content/drive/MyDrive/Imaging Science MS/Applied ML for Remote Sensing/Project/Data/SCAN Data/lindsay_20-25.csv'\n",
        "\n",
        "lindsay_df = build_moisture_data(scan_loc,site_name,start_date,end_date,scan_path)"
      ],
      "metadata": {
        "id": "9YMl0fG8iY1W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------- IMPORT DATA FROM Mount Mansfield, VT ---------------------\n",
        "# SCAN site location\n",
        "scan_loc = [-72.83, 44.54] # station coordinates (long,lat)\n",
        "site_name = \"Mount Mansfield, VT\"\n",
        "start_date = '2020-01-01'\n",
        "end_date = '2024-12-31'\n",
        "scan_path = '/content/drive/MyDrive/Imaging Science MS/Applied ML for Remote Sensing/Project/Data/SCAN Data/mount_mansfield_20-25.csv'\n",
        "\n",
        "mount_mansfield_df = build_moisture_data(scan_loc,site_name,start_date,end_date,scan_path)"
      ],
      "metadata": {
        "id": "xOaGj25Cj7mx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------- IMPORT DATA FROM Powell Gardens, MO ---------------------\n",
        "# SCAN site location\n",
        "scan_loc = [-94.03, 38.87] # station coordinates (long,lat)\n",
        "site_name = \"Powell Gardens, MO\"\n",
        "start_date = '2020-01-01'\n",
        "end_date = '2024-12-31'\n",
        "scan_path = '/content/drive/MyDrive/Imaging Science MS/Applied ML for Remote Sensing/Project/Data/SCAN Data/powell_gardens_20-25.csv'\n",
        "\n",
        "powell_gardens_df = build_moisture_data(scan_loc,site_name,start_date,end_date,scan_path)"
      ],
      "metadata": {
        "id": "uEwYyOoqVk7Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------- IMPORT DATA FROM Tule Valley, UT ---------------------\n",
        "# SCAN site location\n",
        "scan_loc = [-113.46, 39.24] # station coordinates (long,lat)\n",
        "site_name = \"Tule Valley, UT\"\n",
        "start_date = '2020-01-01'\n",
        "end_date = '2024-12-31'\n",
        "scan_path = '/content/drive/MyDrive/Imaging Science MS/Applied ML for Remote Sensing/Project/Data/SCAN Data/tule_valley_20-25.csv'\n",
        "\n",
        "tule_valley_df = build_moisture_data(scan_loc,site_name,start_date,end_date,scan_path)"
      ],
      "metadata": {
        "id": "_PJuELNrXBjC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------- IMPORT DATA FROM Schell-Osage, MO ---------------------\n",
        "# SCAN site location\n",
        "scan_loc = [-94.04, 37.99] # station coordinates (long,lat)\n",
        "site_name = \"Schell-Osage, MO\"\n",
        "start_date = '2020-01-01'\n",
        "end_date = '2024-12-31'\n",
        "scan_path = '/content/drive/MyDrive/Imaging Science MS/Applied ML for Remote Sensing/Project/Data/SCAN Data/schell_osage_20-25.csv'\n",
        "\n",
        "schell_osage_df = build_moisture_data(scan_loc,site_name,start_date,end_date,scan_path)"
      ],
      "metadata": {
        "id": "ytbZojrxbzXq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------- IMPORT DATA FROM Ku-Nesa, KS ---------------------\n",
        "# SCAN site location\n",
        "scan_loc = [-95.19,\t39.05] # station coordinates (long,lat)\n",
        "site_name = \"Ku-Nesa, KS\"\n",
        "start_date = '2020-01-01'\n",
        "end_date = '2024-12-31'\n",
        "scan_path = '/content/drive/MyDrive/Imaging Science MS/Applied ML for Remote Sensing/Project/Data/SCAN Data/ku_nesa_20-25.csv'\n",
        "\n",
        "ku_nesa_df = build_moisture_data(scan_loc,site_name,start_date,end_date,scan_path)"
      ],
      "metadata": {
        "id": "aOmQHtiXgWl3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------- IMPORT DATA FROM Conrad Ag Rc, MT ---------------------\n",
        "# SCAN site location\n",
        "scan_loc = [-111.92, 48.3] # station coordinates (long,lat)\n",
        "site_name = \"Conrad Ag Rc, MT\"\n",
        "start_date = '2020-01-01'\n",
        "end_date = '2024-12-31'\n",
        "scan_path = '/content/drive/MyDrive/Imaging Science MS/Applied ML for Remote Sensing/Project/Data/SCAN Data/conrad_ag_20-25.csv'\n",
        "\n",
        "conrad_ag_df = build_moisture_data(scan_loc,site_name,start_date,end_date,scan_path)"
      ],
      "metadata": {
        "id": "4rTWTotf1XIA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------- IMPORT DATA FROM UW Platteville, WI ---------------------\n",
        "# SCAN site location\n",
        "scan_loc = [-90.39, 42.71] # station coordinates (long,lat)\n",
        "site_name = \"Platteville, WI\"\n",
        "start_date = '2020-01-01'\n",
        "end_date = '2024-12-31'\n",
        "scan_path = '/content/drive/MyDrive/Imaging Science MS/Applied ML for Remote Sensing/Project/Data/SCAN Data/uw_platteville_20-25.csv'\n",
        "\n",
        "uw_platteville_df = build_moisture_data(scan_loc,site_name,start_date,end_date,scan_path)"
      ],
      "metadata": {
        "id": "p_nY5dvB7zOK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------- IMPORT DATA FROM Starkville, MS ---------------------\n",
        "# SCAN site location\n",
        "scan_loc = [-88.78, 33.47] # station coordinates (long,lat)\n",
        "site_name = \"Starkville, MS\"\n",
        "start_date = '2020-01-01'\n",
        "end_date = '2024-12-31'\n",
        "scan_path = '/content/drive/MyDrive/Imaging Science MS/Applied ML for Remote Sensing/Project/Data/SCAN Data/starkville_20-25.csv'\n",
        "\n",
        "starkville_df = build_moisture_data(scan_loc,site_name,start_date,end_date,scan_path)"
      ],
      "metadata": {
        "id": "2imzUm7AS20G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------- IMPORT DATA FROM CMRB LTAR-MO, MO ---------------------\n",
        "# SCAN site location\n",
        "scan_loc = [-92.12, 39.23] # station coordinates (long,lat)\n",
        "site_name = \"CMRB LTAR-MO, MO\"\n",
        "start_date = '2020-01-01'\n",
        "end_date = '2024-12-31'\n",
        "scan_path = '/content/drive/MyDrive/Imaging Science MS/Applied ML for Remote Sensing/Project/Data/SCAN Data/crmb_ltar_20-25.csv'\n",
        "\n",
        "crmb_ltar_df = build_moisture_data(scan_loc,site_name,start_date,end_date,scan_path)"
      ],
      "metadata": {
        "id": "rE4v5-P0UUit"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------- APPEND NEW DATA TO EXISTING DATASET ----------------------\n",
        "import pandas as pd\n",
        "data = pd.read_csv('/content/drive/MyDrive/Imaging Science MS/Applied ML for Remote Sensing/Project/Data/Extracted Data/soil_moisture_data.csv', parse_dates=['date'])\n",
        "new_data = pd.read_csv('/content/new_data.csv', parse_dates=['date'])\n",
        "\n",
        "result = pd.concat([data, new_data], ignore_index=True)\n",
        "\n",
        "# Save to csv\n",
        "result.to_csv('/content/drive/MyDrive/Imaging Science MS/Applied ML for Remote Sensing/Project/Data/Extracted Data/soil_moisture_data.csv', index=False)\n",
        "\n",
        "print(result.shape)"
      ],
      "metadata": {
        "id": "tvUmluRiJKq2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exploratory Data Analysis (EDA)"
      ],
      "metadata": {
        "id": "JYiDrua3A5Nc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# EDA FUNCTIONS\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import xgboost as xgb\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.metrics import mean_absolute_error, r2_score\n",
        "import seaborn as sns\n",
        "from scipy.stats import skew, kurtosis\n",
        "\n",
        "# ----------- Calculate Band Statistics Function----------\n",
        "def calculate_band_statistics(data):\n",
        "\n",
        "    if len(data.shape) > 2:\n",
        "        data = data.reshape(-1, data.shape[2])\n",
        "\n",
        "    data_mean = np.mean(data,axis=0)\n",
        "    data_std = np.std(data,axis=0)\n",
        "    data_min = np.min(data,axis=0)\n",
        "    data_max = np.max(data,axis=0)\n",
        "    data_q1 = np.percentile(data,25,axis=0)\n",
        "    data_median = np.median(data,axis=0)\n",
        "    data_q3 = np.percentile(data,75,axis=0)\n",
        "    data_skew = skew(data,axis=0)\n",
        "    data_kurt = kurtosis(data,axis=0)\n",
        "\n",
        "    stats = np.vstack([data_mean,data_std,data_min,data_max,data_q1,data_median,data_q3,data_skew,data_kurt]).T\n",
        "\n",
        "    return stats\n",
        "\n",
        "  # ----------------Correlation Function--------------------\n",
        "def correlation_matrix(data,wl,stats):\n",
        "\n",
        "    # reshape data where rows and pixels and cols are bands\n",
        "    if len(data.shape) > 2:\n",
        "        reshaped_data = data.reshape(-1, data.shape[2])\n",
        "    else:\n",
        "        reshaped_data = data\n",
        "\n",
        "    # Calculate the covariance matrix between each band\n",
        "    cor = np.zeros((reshaped_data.shape[1],reshaped_data.shape[1]))\n",
        "\n",
        "    for i in range(reshaped_data.shape[1]):\n",
        "        for j in range(reshaped_data.shape[1]):\n",
        "            cor[i,j] = ((np.sum((reshaped_data[:,i] - stats[i,0]) * (reshaped_data[:,j] - stats[j,0]))) / reshaped_data.shape[0]) / (stats[i,1]*stats[j,1])\n",
        "\n",
        "    # Round to 3 decimal places\n",
        "    cor_format = np.round(cor, 3)\n",
        "\n",
        "    # Display the array as a heatmap\n",
        "    fig, ax = plt.subplots(figsize=(6,5))\n",
        "    cax = ax.imshow(cor_format, cmap='coolwarm',vmin=-1, vmax=1)\n",
        "\n",
        "    # Add a colorbar\n",
        "    plt.colorbar(cax)\n",
        "\n",
        "    # Set evenly spaced tick locations\n",
        "    max_ticks = 15  # Maximum number of ticks to display\n",
        "    num_bands = cor_format.shape[0]  # Number of spectral bands\n",
        "\n",
        "    # Dont set more ticks than available bands\n",
        "    num_ticks = min(max_ticks, num_bands)\n",
        "\n",
        "    # Generate tick positions dynamically\n",
        "    tick_positions = np.linspace(0, num_bands - 1, num=num_ticks, dtype=int)\n",
        "    ax.set_xticks(tick_positions)\n",
        "    ax.set_yticks(tick_positions)\n",
        "\n",
        "    # Set labels corresponding to selected tick positions\n",
        "    ax.set_xticklabels([f\"{wl[i]}\" for i in tick_positions], rotation=45)\n",
        "    ax.set_yticklabels([f\"{wl[i]}\" for i in tick_positions], rotation=45)\n",
        "\n",
        "    plt.title(\"Correlation Matrix\")\n",
        "    plt.xlabel(\"Features\")\n",
        "    plt.ylabel(\"Features\")\n",
        "    plt.show()\n",
        "\n",
        "    return cor\n",
        "\n",
        "# -------Function to compute feature/target correlation--------\n",
        "def feature_target_correlation(data,target):\n",
        "    # reshape data where rows and pixels and cols are bands\n",
        "    if len(data.shape) > 2:\n",
        "        reshaped_data = data.reshape(-1, data.shape[2])\n",
        "    target = target.reshape(-1, 1)\n",
        "    r = np.sum((data - data.mean(axis=0)) * (target.reshape(-1, 1) - target.mean()), axis=0) / \\\n",
        "        np.sqrt(np.sum((data - data.mean(axis=0)) ** 2, axis=0) * np.sum((target - target.mean()) ** 2))\n",
        "    return r\n",
        "\n",
        "# ----------- Plot Band Statistics Function----------\n",
        "def stats_plot(stats,band_names):\n",
        "\n",
        "    # Format the matrix to show only 3 significant digits\n",
        "    stats_format = np.round(stats, 3)  # Round to 3 decimal places\n",
        "    stats_format = stats_format.astype(str)  # Convert to string for table\n",
        "    col_labels = [\"Mean\", \"Std\", \"Min\", \"Max\", \"Q1\", 'Median', 'Q3', 'Skew', 'Kurt']\n",
        "\n",
        "    # Show statistics as a table\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.axis(\"tight\")\n",
        "    ax.axis(\"off\")\n",
        "\n",
        "    # Add the table\n",
        "    table = ax.table(cellText=stats_format, loc=\"center\", cellLoc=\"center\",rowLabels=band_names, colLabels=col_labels)\n",
        "    table.auto_set_font_size(False)\n",
        "    table.set_fontsize(7)\n",
        "\n",
        "    plt.title(\"Band Statistics\", loc='center', fontsize=16, pad=0.1)\n",
        "    plt.show()\n",
        "\n",
        "# ----------- Feature Target Correlation Density Plot Function----------\n",
        "def feature_target_density(data,target):\n",
        "  import matplotlib.pyplot as plt\n",
        "  import numpy as np\n",
        "  from scipy.stats import gaussian_kde\n",
        "\n",
        "  # plot relationship of each feature with target color by density\n",
        "  fig, ax = plt.subplots(2, 4, figsize=(12, 6))\n",
        "  axes = ax.flatten()\n",
        "\n",
        "  for i in range(len(feature_names)):\n",
        "      x = features[:, i]\n",
        "      y = target\n",
        "\n",
        "      # Calculate point density\n",
        "      xy = np.vstack([x, y])\n",
        "      z = gaussian_kde(xy)(xy)\n",
        "\n",
        "      sc = axes[i].scatter(x, y, c=z, s=15, edgecolors='none', cmap='viridis')\n",
        "      axes[i].set_title(f'{feature_names[i]}')\n",
        "      axes[i].set_xlabel('Feature Value')\n",
        "      axes[i].set_ylabel('Soil Moisture')\n",
        "\n",
        "  plt.suptitle('Relationship of Soil Moisture Per Feature')\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "JT-xOxUqC-A8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------- PERFORM EDA --------------------------------\n",
        "\n",
        "# Read in extracted soil moisture data\n",
        "data = pd.read_csv('/content/drive/MyDrive/Imaging Science MS/Applied ML for Remote Sensing/Project/Data/Extracted Data/soil_moisture_data.csv', parse_dates=['date'])\n",
        "\n",
        "# Scale soil moisture values from percentage to volume ratio\n",
        "data['soil_moisture'] = data['soil_moisture'] / 100\n",
        "data['sm_smap'] = data['sm_smap'] / 100\n",
        "\n",
        "# Separate into target and features\n",
        "date = data['date']\n",
        "target = data['soil_moisture'].to_numpy()\n",
        "reference = data['sm_smap'].to_numpy()\n",
        "features = data.drop(columns=['soil_moisture', 'date','sm_smap'])\n",
        "feature_names = list(features.columns)\n",
        "features = features.to_numpy()\n",
        "print(len(feature_names))\n",
        "\n",
        "print(\"Target Shape\", target.shape)\n",
        "print(\"Features Shape\", features.shape)\n",
        "\n",
        "# Histogram of soil moisture\n",
        "plt.hist(target, bins=50,edgecolor='black')\n",
        "plt.xlabel('Soil Moisture')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Histogram of Soil Moisture')\n",
        "plt.show()\n",
        "\n",
        "# Calculate band statistics\n",
        "stats = calculate_band_statistics(features)\n",
        "\n",
        "# Plot the statistics of each feature\n",
        "stats_plot(stats, feature_names)\n",
        "\n",
        "# plot correlation with target\n",
        "r = feature_target_correlation(features,target)\n",
        "plt.bar(feature_names, r,edgecolor='black')\n",
        "plt.title(\"Feature-Target Correlation\")\n",
        "plt.xlabel(\"Feature Name\")\n",
        "plt.ylabel(\"Correlation\")\n",
        "plt.show()\n",
        "\n",
        "# plot relationship of each feature with target\n",
        "feature_target_density(features,target)\n",
        "\n",
        "# Show correlation matrix\n",
        "print(list(range(features.shape[1])))\n",
        "cor = correlation_matrix(features,feature_names,stats)\n"
      ],
      "metadata": {
        "id": "f8ZY7O6TkP4f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#-------------------------- DATA PRE-PROCESSING ---------------------------------\n",
        "\n",
        "# Make temp array so we can compare with SMAP predictions\n",
        "temp_array = np.hstack((features, reference.reshape(-1,1)))\n",
        "\n",
        "# Divide into training and testing splits\n",
        "X_train, X_test, y_train, y_test = train_test_split(temp_array, target, test_size=0.2, random_state=42)\n",
        "\n",
        "# Pull off the SMAP predictions to use later\n",
        "ref_train = X_train[:, -1]\n",
        "ref_test = X_test[:, -1]\n",
        "X_train = X_train[:, :-1]\n",
        "X_test = X_test[:, :-1]\n",
        "\n",
        "# Standardize data according to training\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "print(\"Training Data Shape\", X_train.shape)\n",
        "print(\"Testing Data Shape\", X_test.shape)\n",
        "\n",
        "# Histogram of splits\n",
        "fig, ax = plt.subplots(1,2,figsize=(10,4))\n",
        "ax[0].hist(y_train, bins=50,edgecolor='black')\n",
        "ax[0].set_xlabel('Soil Moisture %')\n",
        "ax[0].set_ylabel('Frequency')\n",
        "ax[0].set_title('Histogram of Soil Moisture (Training)')\n",
        "ax[1].hist(y_test, bins=50,edgecolor='black')\n",
        "ax[1].set_xlabel('Soil Moisture %')\n",
        "ax[1].set_ylabel('Frequency')\n",
        "ax[1].set_title('Histogram of Soil Moisture (Testing)')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "LEebEkMPCDUq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Regression Using XGBoost"
      ],
      "metadata": {
        "id": "xEkPwBnOAZ0t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------- TRAIN A REGRESSION MODEL USING XGBOOST ----------------------\n",
        "import xgboost as xgb\n",
        "import sklearn\n",
        "\n",
        "# -------Function to compute regression metrics--------\n",
        "def compute_regression_metrics(y_true, y_pred):\n",
        "    mae = sklearn.metrics.mean_absolute_error(y_true, y_pred)\n",
        "    r2 = sklearn.metrics.r2_score(y_true, y_pred)\n",
        "    std_residuals = np.std(y_true - y_pred)\n",
        "\n",
        "    diff = y_true - y_pred\n",
        "    bias = np.mean(diff)\n",
        "    rmse = np.sqrt(np.mean(diff ** 2))\n",
        "    ubrmse = np.sqrt(np.mean((diff - bias) ** 2))\n",
        "    return mae, r2, std_residuals, bias, rmse, ubrmse\n",
        "\n",
        "params = {\n",
        "    'objective': 'reg:squarederror',   # Loss function\n",
        "    'eval_metric': 'mae',              # Mean aveage error\n",
        "    'max_depth': 5,                    # Depth of trees\n",
        "    'eta': 0.1,                        # Learning rate\n",
        "    'seed': 42                         # For reproducibility\n",
        "}\n",
        "\n",
        "# Convert to DMatrix for XGBoost\n",
        "dtrain = xgb.DMatrix(X_train, label=y_train, feature_names=feature_names)\n",
        "dtest = xgb.DMatrix(X_test, feature_names=feature_names)\n",
        "\n",
        "# Train model\n",
        "model = xgb.train(params, dtrain, num_boost_round=100)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = model.predict(dtest)\n",
        "\n",
        "# Calculate metrics\n",
        "y_true = y_test\n",
        "residuals = y_true - y_pred\n",
        "mae, r2, std_residuals, bias, rmse, ubrmse = compute_regression_metrics(y_true, y_pred)\n",
        "\n",
        "#-------------------Plot results on TEST data------------------------\n",
        "fig,ax = plt.subplots(1,2,figsize=(15,4))\n",
        "# Regression Plot\n",
        "ax[0].scatter(y_true, y_pred, alpha=0.5,edgecolors='black',label='Model Prediction') # model predictions\n",
        "ax[0].plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'r--', lw=2)\n",
        "ax[0].set_xlabel('Actual Soil Moisture (m³/m³)')\n",
        "ax[0].set_ylabel('Predicted Soil Moisture (m³/m³)')\n",
        "ax[0].set_title('Regression Plot')\n",
        "textstr = f\"MAE: {mae:.3f}\\nR²: {r2:.3f}\\nRMSE: {rmse:.3f}\\nubRMSE: {ubrmse:.3f}\"\n",
        "ax[0].text(0.05, 0.95, textstr,\n",
        "           transform=ax[0].transAxes,\n",
        "           verticalalignment='top',\n",
        "           horizontalalignment='left',\n",
        "           fontsize=10)\n",
        "\n",
        "# Residuals Plot\n",
        "ax[1].scatter(y_pred, residuals, color='g',alpha=0.5,edgecolors='black')\n",
        "ax[1].axhline(y=0, color='r', linestyle='--')\n",
        "ax[1].set_xlabel(\"Predicted\")\n",
        "ax[1].set_ylabel(\"Residuals\")\n",
        "ax[1].set_title(\"Residual Plot\")\n",
        "textstr = f\"Std Residuals: {std_residuals:.2f}\"\n",
        "ax[1].text(0.05, 0.95, textstr,\n",
        "           transform=ax[1].transAxes,\n",
        "           verticalalignment='top',\n",
        "           horizontalalignment='left',\n",
        "           fontsize=10)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.suptitle('Testing Data',fontsize=12,fontweight='bold')\n",
        "plt.show()\n",
        "\n",
        "#-------------------Plot results on REFERENCE data------------------------\n",
        "# Calculate metrics\n",
        "residuals = y_true - ref_test\n",
        "mae, r2, std_residuals, bias, rmse, ubrmse = compute_regression_metrics(y_true, ref_test)\n",
        "\n",
        "\n",
        "fig,ax = plt.subplots(1,2,figsize=(15,4))\n",
        "# Regression Plot\n",
        "ax[0].scatter(y_true, ref_test, alpha=0.5,edgecolors='black')\n",
        "ax[0].plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'r--', lw=2)\n",
        "ax[0].set_xlabel('Actual Soil Moisture (m³/m³)')\n",
        "ax[0].set_ylabel('Predicted Soil Moisture (m³/m³)')\n",
        "ax[0].set_title('Regression Plot')\n",
        "textstr = f\"MAE: {mae:.3f}\\nR²: {r2:.3f}\\nRMSE: {rmse:.3f}\\nubRMSE: {ubrmse:.3f}\"\n",
        "ax[0].text(0.05, 0.95, textstr,\n",
        "           transform=ax[0].transAxes,\n",
        "           verticalalignment='top',\n",
        "           horizontalalignment='left',\n",
        "           fontsize=10)\n",
        "\n",
        "# Residuals Plot\n",
        "ax[1].scatter(ref_test, residuals, color='g',alpha=0.5,edgecolors='black')\n",
        "ax[1].axhline(y=0, color='r', linestyle='--')\n",
        "ax[1].set_xlabel(\"Predicted\")\n",
        "ax[1].set_ylabel(\"Residuals\")\n",
        "ax[1].set_title(\"Residual Plot\")\n",
        "textstr = f\"Std Residuals: {std_residuals:.2f}\"\n",
        "ax[1].text(0.05, 0.95, textstr,\n",
        "           transform=ax[1].transAxes,\n",
        "           verticalalignment='top',\n",
        "           horizontalalignment='left',\n",
        "           fontsize=10)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.suptitle('Reference Data (SMAP)',fontsize=12,fontweight='bold')\n",
        "plt.show()\n",
        "\n",
        "#---------------------- Plot feature importance --------------------------------\n",
        "ax = xgb.plot_importance(model, importance_type=\"gain\",show_values=False)\n",
        "fig = plt.gcf()\n",
        "fig.set_size_inches(6,3)\n",
        "plt.title(\"Feature Importance (Gain)\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "print(\"MAE:\", mean_absolute_error(y_true, y_pred))\n",
        "print(\"R2 Score:\", r2_score(y_true, y_pred))\n",
        "print(\"Std Residuals:\", np.std(residuals))\n"
      ],
      "metadata": {
        "id": "7Lx68GszAiz0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Regression Using Multi-Layer Perceptron (MLP)"
      ],
      "metadata": {
        "id": "UcAi65munIWd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------- REGRESSION USING MULTIPLE LAYER PERCEPTRON (MLP)-------------------\n",
        "from sklearn.metrics import mean_absolute_error, r2_score\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "import sklearn\n",
        "\n",
        "# define a range of layer sizes each with one neuron\n",
        "layerList = [\n",
        "    (10,),  # 1 hidden layer, 10 neuron\n",
        "    (10, 10),  # 2 hidden layers, 10 neuron each\n",
        "    (10, 10, 10),  # 3 hidden layers, 10 neuron each\n",
        "    (10, 10, 10, 10),  # 4 hidden layers, 10 neuron each\n",
        "    (10, 10, 10, 10, 10)  # 5 hidden layers, 10 neuron each\n",
        "]\n",
        "r2_train_scores = []\n",
        "\n",
        "fig1, ax1 = plt.subplots(1, len(layerList), figsize=(20, 5))\n",
        "fig2, ax2 = plt.subplots(1, len(layerList), figsize=(20, 5))\n",
        "\n",
        "\n",
        "for i,layer in enumerate(layerList):\n",
        "    # initialize MLP regressor\n",
        "    mlp = MLPRegressor(\n",
        "        hidden_layer_sizes=layerList[i],\n",
        "        activation='relu',\n",
        "        solver='adam',\n",
        "        max_iter=5000,\n",
        "        random_state=42,\n",
        "        learning_rate_init=0.001,\n",
        "        alpha=0.01\n",
        "        )\n",
        "\n",
        "    # train the MLP\n",
        "    mlp.fit(X_train, y_train)\n",
        "\n",
        "    # predict on training set\n",
        "    y_pred_train = mlp.predict(X_train)\n",
        "    mae, r2, std_residuals, bias, rmse, ubrmse = compute_regression_metrics(y_train, y_pred_train)\n",
        "    r2_train_scores.append(r2)\n",
        "\n",
        "    # predict on test set\n",
        "    y_pred_test = mlp.predict(X_test)\n",
        "    mae, r2, std_residuals, bias, rmse, ubrmse = compute_regression_metrics(y_test, y_pred_test)\n",
        "    print(f\"Layer Size: {layer}\")\n",
        "    print(\"RMSE:\", rmse)\n",
        "    print(\"ubRMSE:\", ubrmse)\n",
        "\n",
        "    # regression plot per model\n",
        "    ax1[i].scatter(y_test, y_pred_test, color=\"green\", alpha=0.6, label=\"Predicted Soil Moisture\")\n",
        "    ax1[i].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \"k--\", lw=2,\n",
        "                label='1:1 line')  # Identity line\n",
        "    ax1[i].set_xlabel(\"Actual Soil Moisture\")\n",
        "    ax1[i].set_ylabel(\"Predicted Soil Moisture\")\n",
        "    ax1[i].set_title(f\"Testing Set with {len(layer)} Layers\")\n",
        "    ax1[i].grid(True)\n",
        "    # ax1[i].legend()\n",
        "\n",
        "    # Display metrics on plot\n",
        "    test_metrics = f\"MAE: {mae:.4f}\\nR²: {r2:.4f}\\nStd Res: {std_residuals:.4f}\"\n",
        "    ax1[i].text(0.05, 0.95, test_metrics, transform=ax1[i].transAxes, fontsize=10,\n",
        "                verticalalignment='top', bbox=dict(boxstyle=\"round,pad=0.3\", edgecolor=\"black\", facecolor=\"white\"))\n",
        "\n",
        "    # residual plot per model\n",
        "    ax2[i].scatter(y_pred_test, y_test - y_pred_test, color=\"green\", alpha=0.6, label=\"Prediction Residuals\")\n",
        "    ax2[i].axhline(0, color=\"k\", linestyle=\"--\", lw=2, label='0 line')  # Horizontal line at y=0\n",
        "    ax2[i].set_ylabel(\"Residuals\")\n",
        "    ax2[i].set_xlabel(\"Predicted Soil Moisture\")\n",
        "    ax2[i].set_title(f\"Testing Set with {len(layer)} Layers\")\n",
        "    ax2[i].set_ylim(-0.4, 0.4)\n",
        "    ax2[i].grid(True)\n",
        "    # ax2[i].legend()\n",
        "\n",
        "    # Display metrics on Test Set plot\n",
        "    ax2[i].text(0.05, 0.95, test_metrics, transform=ax2[i].transAxes, fontsize=10,\n",
        "                verticalalignment='top', bbox=dict(boxstyle=\"round,pad=0.3\", edgecolor=\"black\", facecolor=\"white\"))\n",
        "\n",
        "fig1.suptitle('Regression Plots for MLP Layers',fontweight='bold')\n",
        "fig2.suptitle('Residual Plots for MLP Layers',fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# plot the effect of accuracy vs number of hidden layers\n",
        "plt.plot(list(range(1,len(layerList)+1)), r2_train_scores,linestyle=\"-\", marker=\".\")\n",
        "plt.axvline(list(range(1,len(layerList)+1))[np.argmax(r2_train_scores)], color=\"r\", linestyle=\"--\",label=\"Max Score\")\n",
        "plt.title(\"Training Accuracy for Number of MLP Layers\")\n",
        "plt.xlabel(\"Number of Layers\")\n",
        "plt.ylabel(\"R²\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "#--------------------- Plot the best performing MLP --------------------------\n",
        "print(f\"Best Model has {layerList[np.argmax(r2_train_scores)]} layers\")\n",
        "\n",
        "# initialize MLP regressor\n",
        "mlp = MLPRegressor(\n",
        "    hidden_layer_sizes=layerList[np.argmax(r2_train_scores)],\n",
        "    activation='relu',\n",
        "    solver='adam',\n",
        "    max_iter=5000,\n",
        "    random_state=42,\n",
        "    learning_rate_init=0.001,\n",
        "    alpha=0.01\n",
        "    )\n",
        "\n",
        "# train the MLP\n",
        "mlp.fit(X_train, y_train)\n",
        "\n",
        "# predict on test set\n",
        "y_pred = mlp.predict(X_test)\n",
        "mae, r2, std_residuals, bias, rmse, ubrmse = compute_regression_metrics(y_test, y_pred_test)\n",
        "\n",
        "# -------------------- PLOT RESULTS ------------------\n",
        "fig,ax = plt.subplots(1,2,figsize=(15,4))\n",
        "# Regression Plot\n",
        "ax[0].scatter(y_true, y_pred, alpha=0.5,edgecolors='black',label='Model Prediction') # model predictions\n",
        "ax[0].plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'r--', lw=2)\n",
        "ax[0].set_xlabel('Actual Soil Moisture (m³/m³)')\n",
        "ax[0].set_ylabel('Predicted Soil Moisture (m³/m³)')\n",
        "ax[0].set_title('Regression Plot')\n",
        "textstr = f\"MAE: {mae:.3f}\\nR²: {r2:.3f}\\nRMSE: {rmse:.3f}\\nubRMSE: {ubrmse:.3f}\"\n",
        "ax[0].text(0.05, 0.95, textstr,\n",
        "           transform=ax[0].transAxes,\n",
        "           verticalalignment='top',\n",
        "           horizontalalignment='left',\n",
        "           fontsize=10)\n",
        "\n",
        "# Residuals Plot\n",
        "ax[1].scatter(y_pred, residuals, color='g',alpha=0.5,edgecolors='black')\n",
        "ax[1].axhline(y=0, color='r', linestyle='--')\n",
        "ax[1].set_xlabel(\"Predicted\")\n",
        "ax[1].set_ylabel(\"Residuals\")\n",
        "ax[1].set_title(\"Residual Plot\")\n",
        "textstr = f\"Std Residuals: {std_residuals:.2f}\"\n",
        "ax[1].text(0.05, 0.95, textstr,\n",
        "           transform=ax[1].transAxes,\n",
        "           verticalalignment='top',\n",
        "           horizontalalignment='left',\n",
        "           fontsize=10)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.suptitle('Testing Data',fontsize=12,fontweight='bold')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ILUYjC-6nOJC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Regression Using a 1D CNN"
      ],
      "metadata": {
        "id": "s5sj5c1F9CY5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import sklearn\n",
        "from sklearn.metrics import mean_absolute_error, r2_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# -------Function to compute regression metrics--------\n",
        "def compute_regression_metrics(y_true, y_pred):\n",
        "    mae = sklearn.metrics.mean_absolute_error(y_true, y_pred)\n",
        "    r2 = sklearn.metrics.r2_score(y_true, y_pred)\n",
        "    std_residuals = np.std(y_true - y_pred)\n",
        "\n",
        "    diff = y_true - y_pred\n",
        "    bias = np.mean(diff)\n",
        "    rmse = np.sqrt(np.mean(diff ** 2))\n",
        "    ubrmse = np.sqrt(np.mean((diff - bias) ** 2))\n",
        "    return mae, r2, std_residuals, bias, rmse, ubrmse\n",
        "\n",
        "# Read in extracted soil moisture data\n",
        "data = pd.read_csv('/content/drive/MyDrive/Imaging Science MS/Applied ML for Remote Sensing/Project/Data/Extracted Data/soil_moisture_data.csv', parse_dates=['date'])\n",
        "\n",
        "# Scale soil moisture values from percentage to volume ratio\n",
        "data['soil_moisture'] = data['soil_moisture'] / 100\n",
        "data['sm_smap'] = data['sm_smap'] / 100\n",
        "\n",
        "# Separate into target and features\n",
        "date = data['date']\n",
        "target = data['soil_moisture'].to_numpy()\n",
        "reference = data['sm_smap'].to_numpy()\n",
        "features = data.drop(columns=['soil_moisture', 'date','sm_smap'])\n",
        "\n",
        "feature_names = list(features.columns)\n",
        "features = features.to_numpy()\n",
        "print(len(feature_names))\n",
        "\n",
        "print(\"Target Shape\", target.shape)\n",
        "print(\"Features Shape\", features.shape)\n",
        "\n",
        "# Training and testing splits\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)\n",
        "\n",
        "# Testing an validation splits\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=42)\n",
        "\n",
        "# Step 1: Standard scaling based on training data only\n",
        "scaler = StandardScaler() # initialize the scaler\n",
        "scaler.fit(X_train) # fit to only the training data\n",
        "X_train = scaler.transform(X_train) # scale training data\n",
        "X_val = scaler.transform(X_val) # scale validation data\n",
        "X_test = scaler.transform(X_test) # scale test data\n",
        "\n",
        "# Target (y) scaling\n",
        "scaler_y = StandardScaler()\n",
        "y_train = scaler_y.fit_transform(y_train.reshape(-1, 1)).flatten()\n",
        "y_val = scaler_y.transform(y_val.reshape(-1, 1)).flatten()\n",
        "y_test = scaler_y.transform(y_test.reshape(-1, 1)).flatten()\n",
        "\n",
        "#------------------- Create custom dataset class -----------------------\n",
        "class SoilDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "      # Convert numpy arrays to torch tensors with the correct dtype\n",
        "        self.X = torch.tensor(X, dtype=torch.float32)\n",
        "        self.y = torch.tensor(y, dtype=torch.float32)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        X_sample = self.X[idx].unsqueeze(0)\n",
        "        y_sample = self.y[idx].unsqueeze(0)\n",
        "        return X_sample, y_sample\n",
        "\n",
        "# Step 3: Instantiate datasets\n",
        "train_dataset = SoilDataset(X_train, y_train)\n",
        "val_dataset = SoilDataset(X_val, y_val)\n",
        "test_dataset = SoilDataset(X_test, y_test)\n",
        "\n",
        "# Step 4: Create DataLoaders\n",
        "from torch.utils.data import DataLoader\n",
        "batch_size = 64\n",
        "num_workers = 2\n",
        "prefetch_factor = 3\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size,shuffle=True,num_workers=num_workers,pin_memory=True,prefetch_factor=prefetch_factor)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size,shuffle=True,num_workers=num_workers,pin_memory=True,prefetch_factor=prefetch_factor)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size,shuffle=True,num_workers=num_workers,pin_memory=True,prefetch_factor=prefetch_factor)\n",
        "\n",
        "print(\"train dataset size is:\", len(train_dataset))\n",
        "print(\"val dataset size is:\", len(val_dataset))\n",
        "print(\"test dataset size is:\", len(test_dataset))\n",
        "\n",
        "# use GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using:\", device)"
      ],
      "metadata": {
        "id": "Z00X19615uCy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "qvQVkCvlg8QH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna"
      ],
      "metadata": {
        "id": "2pGdJMzFhBJG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "# ------------------- DEFINE OBJECTIVE FUNCTION FOR OPTUNA ---------------------\n",
        "def objective(trial):\n",
        "    # Fix random seed\n",
        "    torch.manual_seed(15)\n",
        "    np.random.seed(15)\n",
        "\n",
        "    # Suggest hyperparameters\n",
        "    lr = trial.suggest_float('lr', 1e-5, 1e-2,log=True)\n",
        "    weight_decay = trial.suggest_float('weight_decay', 1e-6, 1e-2,log=True)\n",
        "    dropout_rate = trial.suggest_float('dropout', 0.0, 0.5)\n",
        "    kernel_size = trial.suggest_int('kernel_size', 3, 7, step=2)\n",
        "    channels_1 = trial.suggest_int('channels_1', 4, 32)\n",
        "    channels_2 = trial.suggest_int('channels_2', 8, 64)\n",
        "    channels_3 = trial.suggest_int('channels_3', 16, 128)\n",
        "    gamma = trial.suggest_float('gamma', 0.1, 0.9)\n",
        "\n",
        "    # Define model with variable architecture\n",
        "    class TrialCNN(nn.Module):\n",
        "        def __init__(self):\n",
        "            super(TrialCNN, self).__init__()\n",
        "            self.net = nn.Sequential(\n",
        "                nn.Conv1d(1, channels_1, kernel_size=kernel_size, padding=kernel_size//2, stride=3), nn.ReLU(),\n",
        "                nn.Conv1d(channels_1, channels_2, kernel_size=kernel_size, padding=kernel_size//2, stride=3), nn.ReLU(),\n",
        "                nn.Conv1d(channels_2, channels_3, kernel_size=kernel_size, padding=kernel_size//2, stride=3), nn.ReLU(),\n",
        "                nn.AdaptiveAvgPool1d(1),\n",
        "                nn.Flatten(),\n",
        "                nn.Dropout(dropout_rate),\n",
        "                nn.Linear(channels_3, 1)\n",
        "            )\n",
        "\n",
        "        def forward(self, x):\n",
        "            return self.net(x)\n",
        "\n",
        "    model = TrialCNN().to(device) # send to gpu\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    # Adam optimizer\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "    # Learning rate scheduler\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=gamma)\n",
        "\n",
        "   # ----------------------- Training loop ----------------------------\n",
        "    best_val_loss = float('inf')\n",
        "    for epoch in range(10):\n",
        "        model.train() # set model to training mode\n",
        "        for X_batch, y_batch in train_loader:\n",
        "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "            optimizer.zero_grad() # reset the gradient\n",
        "            outputs = model(X_batch) # make predictions\n",
        "            loss = criterion(outputs, y_batch) # calculate loss\n",
        "            loss.backward() # backpropogation\n",
        "            optimizer.step() # make a step\n",
        "        scheduler.step() # reduce the learning rate\n",
        "\n",
        "      # -------------------------- Validation -----------------------------\n",
        "        model.eval() # set model to evaluation mode\n",
        "        val_losses = [] # initialize validation loss\n",
        "        with torch.no_grad(): # do not track gradients\n",
        "            for X_batch, y_batch in val_loader:\n",
        "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "                outputs = model(X_batch)\n",
        "                val_loss = criterion(outputs, y_batch).item()\n",
        "                val_losses.append(val_loss) # store the loss for each minibatch\n",
        "\n",
        "        val_loss_mean = np.mean(val_losses) # average the validation loss\n",
        "\n",
        "\n",
        "        if val_loss_mean < best_val_loss:\n",
        "            best_val_loss = val_loss_mean\n",
        "\n",
        "    return best_val_loss\n"
      ],
      "metadata": {
        "id": "Rap0CWx2hC2M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------ OPTUNA STUDY ----------------------------------------\n",
        "\n",
        "# Create Optuna study to minimize the loss\n",
        "study = optuna.create_study(direction='minimize')\n",
        "\n",
        "# Optimize using the objective funtion\n",
        "study.optimize(objective, n_trials=50)\n",
        "\n",
        "print(\"Best Trial:\")\n",
        "print(study.best_trial)\n",
        "\n",
        "# Store best parameters\n",
        "best_params = study.best_params\n",
        "lr = best_params['lr']\n",
        "weight_decay = best_params['weight_decay']\n",
        "dropout_rate = best_params['dropout']\n",
        "kernel_size = best_params['kernel_size']\n",
        "channels_1 = best_params['channels_1']\n",
        "channels_2 = best_params['channels_2']\n",
        "channels_3 = best_params['channels_3']\n",
        "gamma = best_params['gamma']\n",
        "\n",
        "# Print best values\n",
        "print(\"Best Hyperparameters:\")\n",
        "print(f\"Learning Rate: {lr}\")\n",
        "print(f\"Weight Decay: {weight_decay}\")\n",
        "print(f\"Dropout Rate: {dropout_rate}\")\n",
        "print(f\"Kernel Size: {kernel_size}\")\n",
        "print(f\"Channels 1: {channels_1}\")\n",
        "print(f\"Channels 2: {channels_2}\")\n",
        "print(f\"Channels 3: {channels_3}\")\n",
        "print(f\"Gamma: {gamma}\")"
      ],
      "metadata": {
        "id": "Wby2-njhhKCD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------- TRAIN MODEL WITH TUNED HYPERPARAMETERS --------------------\n",
        "\n",
        "class BestSpectralCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BestSpectralCNN, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv1d(1, channels_1, kernel_size=kernel_size, stride=3, padding=kernel_size // 2), nn.ReLU(),\n",
        "            nn.Conv1d(channels_1, channels_2, kernel_size=kernel_size, stride=3, padding=kernel_size // 2), nn.ReLU(),\n",
        "            nn.Conv1d(channels_2, channels_3, kernel_size=kernel_size, stride=3, padding=kernel_size // 2), nn.ReLU(),\n",
        "            nn.AdaptiveAvgPool1d(1),\n",
        "            nn.Flatten(),\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(channels_3, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "model = BestSpectralCNN().to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=gamma)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "# ----------------------- Training loop ----------------------------\n",
        "# keeping track of loss\n",
        "val_loss_history = []\n",
        "train_loss_history = []\n",
        "\n",
        "# For early stopping\n",
        "no_improvement = 0\n",
        "\n",
        "best_val_loss = float('inf')\n",
        "for epoch in range(50):\n",
        "    model.train() # set model to training mode\n",
        "    train_losses = []\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "        optimizer.zero_grad() # reset the gradient\n",
        "        outputs = model(X_batch) # make predictions\n",
        "        loss = criterion(outputs, y_batch) # calculate loss\n",
        "        loss.backward() # backpropogation\n",
        "        optimizer.step() # make a step\n",
        "        train_losses.append(loss.item()) # keep track of training loss\n",
        "\n",
        "    train_loss_mean = np.mean(train_losses)\n",
        "    train_loss_history.append(train_loss_mean)\n",
        "\n",
        "    scheduler.step() # reduce the learning rate\n",
        "\n",
        "# -------------------------- Validation -----------------------------\n",
        "    model.eval() # set model to evaluation mode\n",
        "    val_losses = [] # initialize validation loss\n",
        "    with torch.no_grad(): # do not track gradients\n",
        "        for X_batch, y_batch in val_loader:\n",
        "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "            outputs = model(X_batch)\n",
        "            val_loss = criterion(outputs, y_batch).item()\n",
        "            val_losses.append(val_loss) # store the loss for each minibatch\n",
        "\n",
        "    val_loss_mean = np.mean(val_losses)\n",
        "    val_loss_history.append(val_loss_mean) # store val loss for epoch\n",
        "\n",
        "    if val_loss_mean < best_val_loss:\n",
        "        best_val_loss = val_loss_mean\n",
        "        best_epoch = epoch\n",
        "        torch.save(model.state_dict(), \"best_model.pt\") # save best model\n",
        "        no_improvement = 0\n",
        "    else:\n",
        "      no_improvement = no_improvement + 1\n",
        "      if no_improvement >= 15: # early stopping\n",
        "        print(f\"\\nEarly stopping triggered after {epoch+1} epochs\")\n",
        "        break\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Val Loss: {val_loss_mean:.4f}\")\n",
        "\n",
        "\n",
        "# ---------- PLOT LOSS -------------\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(train_loss_history, '.-',label='Training Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Plot validation loss\n",
        "plt.plot(val_loss_history, 'g.-',label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3WAtmoeAhN6f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------- TEST SET RESULTS ON OPTIMAL MODEL ---------------------------\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "\n",
        "# loading the best model\n",
        "model = BestSpectralCNN().to(device)\n",
        "model.load_state_dict(torch.load('/content/drive/MyDrive/Imaging Science MS/Applied ML for Remote Sensing/Project/Data/Models/best_model.pt'))\n",
        "model.eval()\n",
        "\n",
        "all_preds = []\n",
        "all_targets = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for X_batch, y_batch in test_loader:\n",
        "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "        preds = model(X_batch)\n",
        "        all_preds.append(preds.cpu().numpy())\n",
        "        all_targets.append(y_batch.cpu().numpy())\n",
        "\n",
        "all_preds = np.concatenate(all_preds).flatten()\n",
        "all_targets = np.concatenate(all_targets).flatten()\n",
        "\n",
        "# Unscale predictions and targets before evaluation\n",
        "all_preds = scaler_y.inverse_transform(all_preds.reshape(-1, 1)).flatten()\n",
        "all_targets = scaler_y.inverse_transform(all_targets.reshape(-1, 1)).flatten()\n",
        "\n",
        "# Calculate metrics\n",
        "y_true = all_targets\n",
        "y_pred = all_preds\n",
        "residuals = y_true - y_pred\n",
        "mae, r2, std_residuals, bias, rmse, ubrmse = compute_regression_metrics(y_true, y_pred)\n",
        "\n",
        "print(\"MAE:\", mean_absolute_error(y_true, y_pred))\n",
        "print(\"R2 Score:\", r2_score(y_true, y_pred))\n",
        "print(\"Std Residuals:\", std_residuals)\n",
        "print(\"RMSE:\", rmse)\n",
        "print(\"ubRMSE:\", ubrmse)\n",
        "print(\"Bias:\", bias)\n",
        "\n",
        "# ---------------------- PLOT TEST RESULTS ON OPTIMAL MODEL --------------------\n",
        "fig,ax = plt.subplots(1,2,figsize=(15,4))\n",
        "# Regression Plot\n",
        "ax[0].scatter(y_true, y_pred, alpha=0.5,edgecolors='black')\n",
        "ax[0].plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'r--', lw=2)\n",
        "ax[0].set_xlabel('Actual Soil Moisture (m³/m³)')\n",
        "ax[0].set_ylabel('Predicted Soil Moisture (m³/m³)')\n",
        "ax[0].set_title('Regression Plot')\n",
        "textstr = f\"MAE: {mae:.3f}\\nR²: {r2:.3f}\\nRMSE: {rmse:.3f}\\nubRMSE: {ubrmse:.3f}\"\n",
        "ax[0].text(0.05, 0.95, textstr,\n",
        "           transform=ax[0].transAxes,\n",
        "           verticalalignment='top',\n",
        "           horizontalalignment='left',\n",
        "           fontsize=10)\n",
        "\n",
        "# Residuals Plot\n",
        "ax[1].scatter(y_pred, residuals, color='g',alpha=0.5,edgecolors='black')\n",
        "ax[1].axhline(y=0, color='r', linestyle='--')\n",
        "ax[1].set_xlabel(\"Predicted\")\n",
        "ax[1].set_ylabel(\"Residuals\")\n",
        "ax[1].set_title(\"Residual Plot\")\n",
        "textstr = f\"Std Residuals: {std_residuals:.2f}\"\n",
        "ax[1].text(0.05, 0.95, textstr,\n",
        "           transform=ax[1].transAxes,\n",
        "           verticalalignment='top',\n",
        "           horizontalalignment='left',\n",
        "           fontsize=10)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.suptitle('Testing Data',fontsize=12,fontweight='bold')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gQTgoHsqjNRr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}