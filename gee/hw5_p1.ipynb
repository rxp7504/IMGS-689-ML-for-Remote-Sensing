{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### ðŸš€ Open this Tutorial in Google Colab  \n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/amirhszd/MachineLearning4RemoteSensing/blob/main/hw5/hw5_p1.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ],
      "metadata": {
        "id": "vLyW55aALgV4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Mount Google Drive and Load Data\n",
        "\n",
        "In this step, youâ€™ll mount your Google Drive so that you can access .npy files directly from it. Then, load your training, validation, and test sets using NumPy. Be sure to update the paths to point to your own data files.\n",
        "\n",
        "Make sure to set to runtime session to use GPU otherwise your training is going to be slow."
      ],
      "metadata": {
        "id": "6ltSaqOX_M_2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nup-H_t0DYyB"
      },
      "outputs": [],
      "source": [
        "# import files and mount google drive\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Step 1: Load your .npy files from your Google Drive\n",
        "# TODO: Replace the paths below with the correct paths to your own files\n",
        "folderPath = '/content/drive/MyDrive/Imaging Science MS/Applied ML for Remote Sensing/Homework/HW5 - Neural Networks/Problem 1/data/'\n",
        "X_train = np.load(folderPath + 'landis_chlorophyl_regression_train.npy')\n",
        "y_train = np.load(folderPath + 'landis_chlorophyl_regression_traingt.npy')\n",
        "\n",
        "X_val = np.load(folderPath + 'landis_chlorophyl_regression_val.npy')\n",
        "y_val = np.load(folderPath + 'landis_chlorophyl_regression_valgt.npy')\n",
        "\n",
        "X_test = np.load(folderPath + 'landis_chlorophyl_regression_test.npy')\n",
        "y_test = np.load(folderPath + 'landis_chlorophyl_regression_testgt.npy')\n",
        "\n",
        "print(\"Training set shape:\", X_train.shape)\n",
        "print(\"Validation set shape:\", X_val.shape)\n",
        "print(\"Test set shape:\", X_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# EXPLORATORY DATA ANALYSIS\n",
        "from scipy.stats import skew\n",
        "from scipy.stats import kurtosis\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# ----------- Calculate Band Statistics Function----------\n",
        "def calculate_band_statistics(data):\n",
        "\n",
        "    if len(data.shape) > 2:\n",
        "        data = data.reshape(-1, data.shape[2])\n",
        "\n",
        "    data_mean = np.mean(data,axis=0)\n",
        "    data_std = np.std(data,axis=0)\n",
        "    data_min = np.min(data,axis=0)\n",
        "    data_max = np.max(data,axis=0)\n",
        "    data_q1 = np.percentile(data,25,axis=0)\n",
        "    data_median = np.median(data,axis=0)\n",
        "    data_q3 = np.percentile(data,75,axis=0)\n",
        "    data_skew = skew(data,axis=0)\n",
        "    data_kurt = kurtosis(data,axis=0)\n",
        "\n",
        "    stats = np.vstack([data_mean,data_std,data_min,data_max,data_q1,data_median,data_q3,data_skew,data_kurt]).T\n",
        "\n",
        "    return stats\n",
        "\n",
        "\n",
        "  # ----------------Correlation Function--------------------\n",
        "def correlation_matrix(data,wl,stats):\n",
        "\n",
        "    # reshape data where rows and pixels and cols are bands\n",
        "    if len(data.shape) > 2:\n",
        "        reshaped_data = data.reshape(-1, data.shape[2])\n",
        "    else:\n",
        "        reshaped_data = data\n",
        "\n",
        "    # Calculate the covariance matrix between each band\n",
        "    cor = np.zeros((reshaped_data.shape[1],reshaped_data.shape[1]))\n",
        "\n",
        "    for i in range(reshaped_data.shape[1]):\n",
        "        for j in range(reshaped_data.shape[1]):\n",
        "            cor[i,j] = ((np.sum((reshaped_data[:,i] - stats[i,0]) * (reshaped_data[:,j] - stats[j,0]))) / reshaped_data.shape[0]) / (stats[i,1]*stats[j,1])\n",
        "\n",
        "    # Round to 3 decimal places\n",
        "    cor_format = np.round(cor, 3)\n",
        "\n",
        "    # Display the array as a heatmap\n",
        "    fig, ax = plt.subplots(figsize=(6,5))\n",
        "    cax = ax.imshow(cor_format, cmap='coolwarm',vmin=-1, vmax=1)\n",
        "\n",
        "    # Add a colorbar\n",
        "    plt.colorbar(cax)\n",
        "\n",
        "    # Set evenly spaced tick locations\n",
        "    # Dynamically adjust the number of ticks based on the matrix size\n",
        "    max_ticks = 15  # Maximum number of ticks to display\n",
        "    num_bands = cor_format.shape[0]  # Number of spectral bands\n",
        "\n",
        "    # Dont set more ticks than available bands\n",
        "    num_ticks = min(max_ticks, num_bands)\n",
        "\n",
        "    # Generate tick positions dynamically\n",
        "    tick_positions = np.linspace(0, num_bands - 1, num=num_ticks, dtype=int)\n",
        "    ax.set_xticks(tick_positions)\n",
        "    ax.set_yticks(tick_positions)\n",
        "\n",
        "    # Set labels corresponding to selected tick positions\n",
        "    ax.set_xticklabels([f\"{wl[i]:.0f}\" for i in tick_positions], rotation=45)\n",
        "    ax.set_yticklabels([f\"{wl[i]:.0f}\" for i in tick_positions], rotation=45)\n",
        "\n",
        "    plt.title(\"Correlation Matrix\")\n",
        "    plt.xlabel(\"Bands [nm]\")\n",
        "    plt.ylabel(\"Bands [nm]\")\n",
        "    plt.show()\n",
        "\n",
        "    return cor\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "\n",
        "# Wavelength\n",
        "wl = np.linspace(0.4,2.5,X_train.shape[1]) * 1000\n",
        "print(\"Wavelength shape:\",wl.shape)\n",
        "\n",
        "# Calculate band statistics\n",
        "stats = calculate_band_statistics(X_train)\n",
        "\n",
        "# Show correlation matrix\n",
        "cor = correlation_matrix(X_train,wl,stats)\n",
        "\n",
        "# Plot histogram of each partition\n",
        "fig, ax = plt.subplots(1,3,figsize=(15,4))\n",
        "ax[0].hist(X_train.flatten(),bins=100)\n",
        "ax[0].set_title(\"Training Set\")\n",
        "ax[0].set_xlabel(\"Chlorophyll Content\")\n",
        "ax[0].set_ylabel(\"Frequency\")\n",
        "ax[1].hist(X_val.flatten(),bins=100)\n",
        "ax[1].set_title(\"Validation Set\")\n",
        "ax[1].set_xlabel(\"Chlorophyll Content\")\n",
        "ax[1].set_ylabel(\"Frequency\")\n",
        "ax[2].hist(X_test.flatten(),bins=100)\n",
        "ax[2].set_title(\"Test Set\")\n",
        "ax[2].set_xlabel(\"Chlorophyll Content\")\n",
        "ax[2].set_ylabel(\"Frequency\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "9ExX3sFUIuHp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Standardize the Data & Create Dataset\n",
        "\n",
        "Here, you will standardize your dataset using StandardScaler. This ensures that each feature has zero mean and unit variance. Remember to fit the scaler only on the training data, and then transform all splits using that same scaler.\n",
        "\n",
        "Youâ€™ll define a custom PyTorch Dataset to wrap your data and make it compatible with DataLoader. After that, instantiate the datasets and set up data loaders with appropriate batch size and parallel loading settings (num_workers, prefetch_factor)."
      ],
      "metadata": {
        "id": "lCM6nnOw_U7H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Step 1: Standard scaling based on training data only\n",
        "# TODO: Scale your validation and test sets using statistics from the training set only\n",
        "\n",
        "scaler = StandardScaler() # initialize the scaler\n",
        "scaler.fit(X_train) # fit to only the training data\n",
        "X_train_scaled = scaler.transform(X_train) # scale training data\n",
        "X_val_scaled = scaler.transform(X_val) # scale validation data\n",
        "X_test_scaled = scaler.transform(X_test) # scale test data\n",
        "\n",
        "# Target (y) scaling\n",
        "scaler_y = StandardScaler()\n",
        "y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1, 1)).flatten()\n",
        "y_val_scaled = scaler_y.transform(y_val.reshape(-1, 1)).flatten()\n",
        "y_test_scaled = scaler_y.transform(y_test.reshape(-1, 1)).flatten()\n",
        "\n",
        "\n",
        "\n",
        "# Step 2: Create custom dataset class\n",
        "class ChlorophyllDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "      # TODO: Convert your numpy arrays to torch tensors with the correct dtype\n",
        "        self.X = torch.tensor(X, dtype=torch.float32)\n",
        "        self.y = torch.tensor(y, dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "# Step 3: Instantiate your datasets\n",
        "train_dataset = ChlorophyllDataset(X_train_scaled, y_train_scaled)\n",
        "val_dataset = ChlorophyllDataset(X_val_scaled, y_val_scaled)\n",
        "test_dataset = ChlorophyllDataset(X_test_scaled, y_test_scaled)\n",
        "\n",
        "# Step 4: Create DataLoaders\n",
        "# TODO: Set batch_size, num_workers, and prefetch_factor based on your system\n",
        "from torch.utils.data import DataLoader\n",
        "batch_size = 64\n",
        "num_workers = 2\n",
        "prefetch_factor = 3\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size,shuffle=True,num_workers=num_workers,pin_memory=True,prefetch_factor=prefetch_factor)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size,shuffle=True,num_workers=num_workers,pin_memory=True,prefetch_factor=prefetch_factor)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size,shuffle=True,num_workers=num_workers,pin_memory=True,prefetch_factor=prefetch_factor)\n",
        "\n",
        "print(\"train dataset size is:\", len(train_dataset))\n",
        "print(\"val dataset size is:\", len(val_dataset))\n",
        "print(\"test dataset size is:\", len(test_dataset))\n"
      ],
      "metadata": {
        "id": "X5fXXSWNGnpO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define Your 1D Convolutional Model\n",
        "In this step, youâ€™ll define a custom SpectralCNN model using PyTorchâ€™s nn Module. This model consists of a sequence of 1D convolutional layers with increasing filter sizes and stride to reduce the spectral dimension. The final Linear layer maps the learned features to a single output value for regression.\n",
        "\n",
        "ðŸ’¡ Note: Since your input has shape [batch_size, 425], youâ€™ll need to change the shape accordingly."
      ],
      "metadata": {
        "id": "u6IDq8GW_s6u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch.nn as nn\n",
        "\n",
        "class SpectralCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SpectralCNN, self).__init__()\n",
        "        # 8 layer 1D CNN with kernel size 3 and stride of 2\n",
        "        # stride 2 lowers the dimensionality spectral dimensionality\n",
        "        # relu is the activation function introducing nonlinearity\n",
        "        # the bigger the kernel size the more number of parameters to learn\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv1d(1, 8, kernel_size=3, stride=2), nn.ReLU(),  # [B, 8, 212]\n",
        "            nn.Conv1d(8, 16, kernel_size=3, stride=2), nn.ReLU(), # [B, 16, 105]\n",
        "            nn.Conv1d(16, 32, kernel_size=3, stride=2), nn.ReLU(),# [B, 32, 52]\n",
        "            nn.Conv1d(32, 64, kernel_size=3, stride=2), nn.ReLU(),# [B, 64, 25]\n",
        "            nn.Conv1d(64, 128, kernel_size=3, stride=2), nn.ReLU(),# [B, 128, 12]\n",
        "            nn.Conv1d(128, 128, kernel_size=3, stride=2), nn.ReLU(),# [B, 128, 5]\n",
        "            nn.Conv1d(128, 256, kernel_size=3, stride=2), nn.ReLU(),# [B, 256, 2]\n",
        "            nn.Conv1d(256, 256, kernel_size=2, stride=1), nn.ReLU(),# [B, 256, 1]\n",
        "            nn.Flatten(),\n",
        "            # we are performing regression so output is one value\n",
        "            nn.Linear(256, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # TODO: x is of size Nx425, however, we need to define the number of channels for our 1D data\n",
        "        # in this case there is only 1 channel/feature for our 425 input features\n",
        "        # in 2D space we have N x C_in x H x W.\n",
        "        # in 1D space we should have N x C_in x Length\n",
        "        # modify the retun correctly to reflect this\n",
        "        x = x.unsqueeze(1) # make channels for each feature\n",
        "        return self.net(x)\n",
        "\n"
      ],
      "metadata": {
        "id": "k-ZeLhj7IAJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define the Model, Optimizer, and Training Configuration\n",
        "\n",
        "In this section, you will define the loss function for your regression task, instantiate your model, and set up the optimizer of your choice. Make sure to use weight decay to prevent overfitting.\n",
        "\n",
        "Your training will benefit from learning rate scheduler such as to gradually reduce the learning rate during training. Finally, youâ€™ll move your model to the GPU if available.\n",
        "\n",
        "ðŸ’¡ Tip: Your learning rate and weight decay are hyperparameters!"
      ],
      "metadata": {
        "id": "6MhICEWsOf3_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Define regression LOSS\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "# instantiating the model\n",
        "model = SpectralCNN()\n",
        "\n",
        "# TODO: define optimizer, suggestion is on Adam with weight decay.\n",
        "# keep in mind weight decay and learning rate are hyperpareters\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-8,weight_decay=1e-5)\n",
        "\n",
        "# TODO: you can define a learning rate scheduler at the time of training\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
        "\n",
        "# Optional: use GPU if available\n",
        "# make sure to change your runtime type to GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "print(\"Using:\", device)"
      ],
      "metadata": {
        "id": "g91cEqACI149"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training Loop and Early Stopping\n",
        "\n",
        "In this section, you will implement the full training loop for your model. For each epoch, youâ€™ll:\n",
        "\n",
        "\tâ€¢\tLoop through your training data\n",
        "\tâ€¢\tSend batches to the correct device\n",
        "\tâ€¢\tPerform forward and backward passes\n",
        "\tâ€¢\tUpdate the model using your optimizer\n",
        "\n",
        "After training on each epoch, youâ€™ll evaluate your model on the validation set without computing gradients. Youâ€™ll track the best-performing model based on validation loss and save it using torch.save(). Finally, youâ€™ll step the learning rate scheduler to gradually reduce the learning rate over time.\n",
        "\n",
        "Make sure to plot loss for training and validation sets, this will help you see into overfitting and underfitting.\n",
        "\n",
        "ðŸ›‘ Donâ€™t forget: only update weights during training, and use torch.no_grad() during validation to save memory and speed up evaluation."
      ],
      "metadata": {
        "id": "5auafzgzAjQI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Initialize variables for early stopping\n",
        "best_val_loss = float(\"inf\")\n",
        "best_epoch = -1\n",
        "\n",
        "# keeping track of validation loss\n",
        "val_loss_history = []\n",
        "train_loss_history = []\n",
        "\n",
        "# For early stopping\n",
        "no_improvement = 0\n",
        "\n",
        "# Training loop\n",
        "epochs = 50\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    train_losses = []\n",
        "\n",
        "    # TOOD: keep track of any other regression metric you want per\n",
        "\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        # TODO: send input features and reference target to device\n",
        "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "\n",
        "        # zeroing out previous step gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # make a prediction using the model\n",
        "        outputs = model(X_batch)\n",
        "\n",
        "        # TODO: calculate the loss\n",
        "        loss = criterion(outputs, y_batch)\n",
        "\n",
        "        # TODO: calculate the gradients by calling backward on the loss\n",
        "        loss.backward()\n",
        "\n",
        "        # TODO: take a step by calling step on the optimizer\n",
        "        optimizer.step()\n",
        "\n",
        "        # TODO: keep score of your training loss and any other metric you see fit for\n",
        "        # your regression task\n",
        "        train_losses.append(loss.item())\n",
        "\n",
        "    train_loss_mean = np.mean(train_losses)\n",
        "    train_loss_history.append(train_loss_mean)\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_losses = []\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in val_loader:\n",
        "            # TODO: Same as training but we are not updating weights\n",
        "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(X_batch)\n",
        "            val_loss = criterion(outputs, y_batch)\n",
        "            val_losses.append(val_loss.item())\n",
        "\n",
        "    # TODO: implementing early stopping here.\n",
        "    # keep track of the validation loss and save the model parameters\n",
        "    # when the loss is the lowest for the validation set\n",
        "    val_loss_mean = np.mean(val_losses)\n",
        "    val_loss_history.append(val_loss_mean)\n",
        "    if val_loss_mean < best_val_loss:\n",
        "        best_val_loss = val_loss_mean\n",
        "        best_epoch = epoch\n",
        "        torch.save(model.state_dict(), \"best_model.pt\")\n",
        "        no_improvement = 0\n",
        "    else:\n",
        "      no_improvement = no_improvement + 1\n",
        "      if no_improvement >= 10:\n",
        "        print(f\"\\nEarly stopping triggered after {best_epoch+1} epochs\")\n",
        "        break\n",
        "\n",
        "\n",
        "    # TODO: take a step in the scheduler to update the learning rate\n",
        "    scheduler.step()\n",
        "\n",
        "    # Print summary\n",
        "    print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {np.mean(train_losses):.4f} | Val Loss: {val_loss_mean:.4f} | LR: {scheduler.get_last_lr()[0]:.6f}\")\n",
        "\n",
        "print(f\"\\nBest model saved from epoch {best_epoch+1} with Val Loss: {best_val_loss:.4f}\")\n"
      ],
      "metadata": {
        "id": "ofJ-WIMV9-fm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get predictions and plot your results\n",
        "Get predictions on your test set and plot your regression results for your training and test set."
      ],
      "metadata": {
        "id": "fmbe3TSIG2bx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(train_loss_history, '.-',label='Training Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Plot validation loss\n",
        "plt.plot(val_loss_history, 'g.-',label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "s55hMKMA730t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import mean_absolute_error, r2_score\n",
        "\n",
        "# TODO: we are to check the model's performance on the validation set\n",
        "# load the \"best_model.pt\" and pass your test data to it and get predictions\n",
        "# make sure the model is in eval mode, send the data to device\n",
        "# note: make sure you handled \"shuffle\" properly in your validation/testing set\n",
        "\n",
        "# loading the best model\n",
        "model.load_state_dict(torch.load(\"best_model.pt\", weights_only=True))\n",
        "model.eval()\n",
        "\n",
        "all_preds = []\n",
        "all_targets = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for X_batch, y_batch in val_loader:\n",
        "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "            preds = model(X_batch).squeeze()\n",
        "            all_preds.append(preds.cpu())\n",
        "            all_targets.append(y_batch.cpu())\n",
        "\n",
        "# concatenate everything\n",
        "y_pred = torch.cat(all_preds).numpy()\n",
        "y_true = torch.cat(all_targets).numpy()\n",
        "\n",
        "# Unscale predictions and targets before evaluation\n",
        "y_pred = scaler_y.inverse_transform(y_pred.reshape(-1, 1)).flatten()\n",
        "y_true = scaler_y.inverse_transform(y_true.reshape(-1, 1)).flatten()\n",
        "\n",
        "\n",
        "#TODO: plot regression plot and residual plots for your regression task,\n",
        "# report R2, and MAE along with any other metrics you want\n",
        "# analyze your results and report on your findings.\n",
        "\n",
        "print(\"Size of predicted and true\",y_pred.shape, y_true.shape)\n",
        "\n",
        "# Calculate metrics\n",
        "mae = mean_absolute_error(y_true, y_pred)\n",
        "r2 = r2_score(y_true, y_pred)\n",
        "residuals = y_true - y_pred\n",
        "std_residuals = np.std(residuals)\n",
        "\n",
        "# Plot results on validation data\n",
        "fig,ax = plt.subplots(1,2,figsize=(15,4))\n",
        "# Regression Plot\n",
        "ax[0].scatter(y_true, y_pred, alpha=0.5,edgecolors='black')\n",
        "ax[0].plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'r--', lw=2)\n",
        "ax[0].set_xlabel('Actual Chlorophyll Content')\n",
        "ax[0].set_ylabel('Predicted Chlorophyll Content')\n",
        "ax[0].set_title('Regression Plot')\n",
        "textstr = f\"MAE: {mae:.2f}\\nRÂ²: {r2:.2f}\"\n",
        "ax[0].text(0.05, 0.95, textstr,\n",
        "           transform=ax[0].transAxes,\n",
        "           verticalalignment='top',\n",
        "           horizontalalignment='left',\n",
        "           fontsize=10)\n",
        "\n",
        "# Residuals Plot\n",
        "ax[1].scatter(y_pred, residuals, color='g',alpha=0.5,edgecolors='black')\n",
        "ax[1].axhline(y=0, color='r', linestyle='--')\n",
        "ax[1].set_xlabel(\"Predicted\")\n",
        "ax[1].set_ylabel(\"Residuals\")\n",
        "ax[1].set_title(\"Residual Plot\")\n",
        "textstr = f\"Std Residuals: {std_residuals:.2f}\"\n",
        "ax[1].text(0.05, 0.95, textstr,\n",
        "           transform=ax[1].transAxes,\n",
        "           verticalalignment='top',\n",
        "           horizontalalignment='left',\n",
        "           fontsize=10)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.suptitle('Validation Data',fontsize=12,fontweight='bold')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "print(\"MAE:\", mean_absolute_error(y_true, y_pred))\n",
        "print(\"R2 Score:\", r2_score(y_true, y_pred))\n",
        "print(\"Std Residuals:\", std_residuals)\n"
      ],
      "metadata": {
        "id": "9sGEEF6uRRk_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I am attaching the test results from Xgboost here for your reference! Your tuned model should outpefrom this performance:\n",
        "\n",
        "MAE=3.22, R2=0.96\n",
        "\n",
        "![xgboost](xgboost.png)"
      ],
      "metadata": {
        "id": "K2DHbUVIKFpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hyperparamter Tuning\n",
        "This part is all you. You have a base model that is performing somewhat accurately for you. Grab a package and perform hyperparameter tuning on your hyperparameters. Read Below!!!"
      ],
      "metadata": {
        "id": "MaXz1aWLHG7j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: now that we have a base model that things are working, we perform hyperparameter tuning on validation data\n",
        "\n",
        "# change the learning rate value\n",
        "# number of layers\n",
        "# filter size (the bigger the filter size the more computationally expensive)\n",
        "# instead of ReLU use leaky Relu or Tanh\n",
        "# learning weight decay\n",
        "\n",
        "# using max/average pooling instead of stride 2\n",
        "\n",
        "# icorporating padding\n",
        "# you can log the lowest loss (highest accuracy) on your validation set and report that\n"
      ],
      "metadata": {
        "id": "zkEj1g1OXKD5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.optim import Adam\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "input_size = X_train.shape[1]\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "# Define a new version of our custom class with tunable hyperparameters\n",
        "class SpectralCNN(nn.Module):\n",
        "    def __init__(self, input_size, num_layers=8, base_channels=8, kernel_size=3,stride=1,padding=1,activation=nn.ReLU,pooling=nn.AvgPool1d):\n",
        "        super(SpectralCNN, self).__init__()\n",
        "\n",
        "        layers = [] # dynamic layer size\n",
        "        in_channels = 1 # input\n",
        "        out_channels = base_channels # first output\n",
        "\n",
        "        # Create a new layer for the number of defined layers\n",
        "        for i in range(num_layers):\n",
        "            layers.append(nn.Conv1d(in_channels, out_channels, kernel_size=kernel_size, stride=stride,padding=padding))\n",
        "            layers.append(activation()) # tunable activation function\n",
        "            layers.append(pooling(kernel_size=2, stride=2)) # pooling\n",
        "\n",
        "            # follow the same in/out structure as original class\n",
        "            in_channels = out_channels\n",
        "            out_channels *= 2\n",
        "\n",
        "        # layers.append(nn.Flatten()) # flatten the features for the fully connected layer\n",
        "\n",
        "        self.features = nn.Sequential(*layers) # pass the final list of model layers\n",
        "\n",
        "\n",
        "        # Create a dummy input to figure out final flattened size\n",
        "        with torch.no_grad():\n",
        "          dummy_input = torch.zeros(1, 1, input_size)  # (batch=1, channel=1, length=input_size)\n",
        "          dummy_output = self.features(dummy_input)\n",
        "          self.num_features = dummy_output.view(1, -1).shape[1]  # flatten and get the feature size\n",
        "\n",
        "\n",
        "        # Use the final out_channels from the last layer\n",
        "        # self.regressor = nn.Linear(in_channels, 1)\n",
        "        self.regressor = nn.Linear(self.num_features, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.unsqueeze(1)  # add the channel dimension\n",
        "        x = self.features(x)  # after all convolutions + pooling\n",
        "        x = x.view(x.size(0), -1)  # flatten before regressor\n",
        "        return self.regressor(x)  # final regression output\n"
      ],
      "metadata": {
        "id": "FzpihA3u8gh-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function for training and validation during the hyperparameter tuning\n",
        "def train_and_validate(model, train_loader, val_loader, optimizer, criterion, device, num_epochs=5):\n",
        "\n",
        "  # ----------------------- Training loop ----------------------------\n",
        "      for epoch in range(num_epochs):  # start with few epochs for speed\n",
        "          model.train() # set model to training mode\n",
        "          for X_batch, y_batch in train_loader: # use our training data\n",
        "              X_batch, y_batch = X_batch.to(device), y_batch.to(device) # send to device\n",
        "              optimizer.zero_grad() # reset the gradient\n",
        "              outputs = model(X_batch) # make predictions\n",
        "              loss = criterion(outputs, y_batch) # calculate loss\n",
        "              loss.backward() # backpropogation\n",
        "              optimizer.step() # make a step\n",
        "\n",
        "      #-------------------------- Validation -----------------------------\n",
        "      model.eval() # set model to evaluation mode\n",
        "      val_losses = [] # initialize validation loss\n",
        "      with torch.no_grad(): # do not track gradients\n",
        "          for X_batch, y_batch in val_loader: # use validation data\n",
        "              X_batch, y_batch = X_batch.to(device), y_batch.to(device) # send to device\n",
        "              outputs = model(X_batch) # make predictions\n",
        "              loss = criterion(outputs, y_batch) # calculate loss\n",
        "              val_losses.append(loss.item()) # store the loss for each minibatch\n",
        "      avg_val_loss = np.mean(val_losses) # average the validation loss\n",
        "      return avg_val_loss\n"
      ],
      "metadata": {
        "id": "eEVNGAakNh3p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------- TUNE THE LEARNING RATE -----------------------------\n",
        "\n",
        "# Define an objective function only varying the learning rate\n",
        "def objective_lr(trial):\n",
        "    # Fix random seed\n",
        "    torch.manual_seed(15)\n",
        "    np.random.seed(15)\n",
        "\n",
        "    # Start with a range of learning rates\n",
        "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-2,log=True) # sample learning rates on log scale\n",
        "\n",
        "    # Initialize our spectral 1D CNN model\n",
        "    model = SpectralCNN(input_size=input_size).to(device)\n",
        "\n",
        "    # Set adam optimizer using the learning rates under trial\n",
        "    optimizer = Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    # Use MSE as the loss metric\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    # Train and calculate validation loss\n",
        "    return train_and_validate(model, train_loader, val_loader, optimizer, criterion, device, num_epochs=10)\n",
        "#===============================================================================\n",
        "\n",
        "# Create Optuna study to minimize the loss\n",
        "study = optuna.create_study(direction=\"minimize\")\n",
        "\n",
        "# Optimize using the objective funtion with different learning rates\n",
        "study.optimize(objective_lr, n_trials=10)\n",
        "\n",
        "# Print the best learning rate\n",
        "best_lr = study.best_params[\"lr\"]\n",
        "print(\"Best learning rate:\", best_lr)\n",
        "\n",
        "# Print the best validation loss\n",
        "print(\"Best validation loss:\", study.best_value)"
      ],
      "metadata": {
        "id": "WV7aCUxyK9H4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------- TUNE THE NUMBER OF LAYERS -----------------------------\n",
        "\n",
        "# Define an objective function only varying the layer size\n",
        "def objective_ls(trial):\n",
        "    # Fix random seed\n",
        "    torch.manual_seed(15)\n",
        "    np.random.seed(15)\n",
        "\n",
        "    # # Set a range for number of layers\n",
        "    num_layers = trial.suggest_int(\"num_layers\", 4, 7)\n",
        "\n",
        "    # Initialize our spectral 1D CNN model\n",
        "    model = SpectralCNN(input_size=input_size,num_layers=num_layers).to(device)\n",
        "\n",
        "    # Set adam optimizer using the learning rates under trial\n",
        "    optimizer = Adam(model.parameters(), lr=best_lr)\n",
        "\n",
        "    # Use MSE as the loss metric\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    # Train and calculate validation loss\n",
        "    return train_and_validate(model, train_loader, val_loader, optimizer, criterion, device, num_epochs=5)\n",
        "\n",
        "#===============================================================================\n",
        "\n",
        "# Create Optuna study to minimize the loss\n",
        "study = optuna.create_study(direction=\"minimize\")\n",
        "\n",
        "# Optimize using the objective funtion with different layer sizes\n",
        "study.optimize(objective_ls, n_trials=10)\n",
        "\n",
        "# Print the best layer size\n",
        "best_ls = study.best_params[\"num_layers\"]\n",
        "print(\"Best number of layers:\", best_ls)\n",
        "\n",
        "# Print the best validation loss\n",
        "print(\"Best validation loss:\", study.best_value)"
      ],
      "metadata": {
        "id": "9bmUoz1sIC6i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------- TUNE THE FILTER SIZE -----------------------------\n",
        "\n",
        "# Define an objective function only varying the filter size\n",
        "def objective_fs(trial):\n",
        "    # Fix random seed\n",
        "    torch.manual_seed(15)\n",
        "    np.random.seed(15)\n",
        "\n",
        "    # Set a range for kernel sizes\n",
        "    kernel_size = trial.suggest_int(\"kernel_size\", 2,5)\n",
        "\n",
        "    # Initialize our spectral 1D CNN model (set padding=2 to use larger filters)\n",
        "    model = SpectralCNN(input_size=input_size,num_layers=best_ls,kernel_size=kernel_size,padding=2).to(device)\n",
        "\n",
        "    # Set adam optimizer using the learning rates under trial\n",
        "    optimizer = Adam(model.parameters(), lr=best_lr)\n",
        "\n",
        "    # Use MSE as the loss metric\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    # Train and calculate validation loss\n",
        "    return train_and_validate(model, train_loader, val_loader, optimizer, criterion, device, num_epochs=5)\n",
        "\n",
        "#===============================================================================\n",
        "\n",
        "# Create Optuna study to minimize the loss\n",
        "study = optuna.create_study(direction=\"minimize\")\n",
        "\n",
        "# Optimize using the objective funtion with different filter sizes\n",
        "study.optimize(objective_fs, n_trials=5)\n",
        "\n",
        "# Print the best filter size\n",
        "best_fs = study.best_params[\"kernel_size\"]\n",
        "print(\"Best filter size:\", best_fs)\n",
        "\n",
        "# Print the best validation loss\n",
        "print(\"Best validation loss:\", study.best_value)"
      ],
      "metadata": {
        "id": "L1i-UL_lJODU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------- TUNE THE ACTIVATION FUNCTION -----------------------------\n",
        "\n",
        "# Define an objective function only varying the activation function\n",
        "def objective_af(trial):\n",
        "    # Fix random seed\n",
        "    torch.manual_seed(15)\n",
        "    np.random.seed(15)\n",
        "\n",
        "    # Set a range for activation functions\n",
        "    activation_name = trial.suggest_categorical(\"activation\", [\"ReLU\", \"LeakyReLU\", \"Tanh\"])\n",
        "\n",
        "    activation_fn = {\n",
        "      \"ReLU\": nn.ReLU,\n",
        "      \"LeakyReLU\": nn.LeakyReLU,\n",
        "      \"Tanh\": nn.Tanh\n",
        "    }[activation_name]\n",
        "\n",
        "    # Initialize our spectral 1D CNN model\n",
        "    model = SpectralCNN(input_size=input_size,num_layers=best_ls,kernel_size=best_fs,padding=2,activation=activation_fn).to(device)\n",
        "\n",
        "    # Set adam optimizer using the learning rates under trial\n",
        "    optimizer = Adam(model.parameters(), lr=best_lr)\n",
        "\n",
        "    # Use MSE as the loss metric\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    # Train and calculate validation loss\n",
        "    return train_and_validate(model, train_loader, val_loader, optimizer, criterion, device, num_epochs=5)\n",
        "\n",
        "#===============================================================================\n",
        "\n",
        "# Create Optuna study to minimize the loss\n",
        "study = optuna.create_study(direction=\"minimize\")\n",
        "\n",
        "# Optimize using the objective funtion with different activation functions\n",
        "study.optimize(objective_af, n_trials=5)\n",
        "\n",
        "# Print the best activation function\n",
        "best_activation = study.best_params[\"activation\"]\n",
        "print(\"Best activation function:\", best_activation)\n",
        "\n",
        "# Print the best validation loss\n",
        "print(\"Best validation loss:\", study.best_value)"
      ],
      "metadata": {
        "id": "XBeviYfOQYgw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------- TUNE THE WEIGHT DECAY -----------------------------\n",
        "\n",
        "# Define an objective function only varying the weight decay\n",
        "def objective_wd(trial):\n",
        "    # Fix random seed\n",
        "    torch.manual_seed(15)\n",
        "    np.random.seed(15)\n",
        "\n",
        "    # Set a range for weight decay\n",
        "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-2,log=True)\n",
        "\n",
        "    activation_fn = {\n",
        "      \"ReLU\": nn.ReLU,\n",
        "      \"LeakyReLU\": nn.LeakyReLU,\n",
        "      \"Tanh\": nn.Tanh\n",
        "    }[best_activation]\n",
        "\n",
        "    # Initialize our spectral 1D CNN model\n",
        "    model = SpectralCNN(input_size=input_size,num_layers=best_ls,kernel_size=best_fs,padding=2,activation=activation_fn).to(device)\n",
        "\n",
        "    # Set adam optimizer using the learning rates under trial\n",
        "    optimizer = Adam(model.parameters(), lr=best_lr,weight_decay=weight_decay)\n",
        "\n",
        "    # Use MSE as the loss metric\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    # Train and calculate validation loss\n",
        "    return train_and_validate(model, train_loader, val_loader, optimizer, criterion, device, num_epochs=5)\n",
        "\n",
        "#===============================================================================\n",
        "\n",
        "# Create Optuna study to minimize the loss\n",
        "study = optuna.create_study(direction=\"minimize\")\n",
        "\n",
        "# Optimize using the objective funtion with different weight decay\n",
        "study.optimize(objective_wd, n_trials=5)\n",
        "\n",
        "# Print the best activation function\n",
        "best_wd = study.best_params[\"weight_decay\"]\n",
        "print(\"Best weight decay:\", best_wd)\n",
        "\n",
        "# Print the best validation loss\n",
        "print(\"Best validation loss:\", study.best_value)"
      ],
      "metadata": {
        "id": "gl8NrxjsTQ3a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define regression loss\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "# instantiating the model with tuned hyperparameters\n",
        "model = SpectralCNN(input_size=input_size,num_layers=best_ls,kernel_size=best_fs,padding=2,activation=nn.ReLU)\n",
        "\n",
        "# define optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=best_lr, betas=(0.9, 0.999), eps=1e-8, weight_decay=best_wd)\n",
        "\n",
        "# you can define a learning rate scheduler at the time of training\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
        "\n",
        "# use GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "print(\"Using:\", device)"
      ],
      "metadata": {
        "id": "esv5sQjCX8bI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------- TRAIN MODEL WITH TUNED HYPERPARAMETERS --------------------\n",
        "\n",
        "# Initialize variables for early stopping\n",
        "best_val_loss = float(\"inf\")\n",
        "best_epoch = -1\n",
        "\n",
        "# keeping track of validation loss\n",
        "val_loss_history = []\n",
        "train_loss_history = []\n",
        "\n",
        "# For early stopping\n",
        "no_improvement = 0\n",
        "\n",
        "# Training loop\n",
        "epochs = 50\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    train_losses = []\n",
        "\n",
        "    # TOOD: keep track of any other regression metric you want per\n",
        "\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        # TODO: send input features and reference target to device\n",
        "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "\n",
        "        # zeroing out previous step gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # send input features and reference target to device\n",
        "        outputs = model(X_batch)\n",
        "\n",
        "        # TODO: calculate the loss\n",
        "        loss = criterion(outputs, y_batch)\n",
        "\n",
        "        # TODO: calculate the gradients by calling backward on the loss\n",
        "        loss.backward()\n",
        "\n",
        "        # TODO: take a step by calling step on the optimizer\n",
        "        optimizer.step()\n",
        "\n",
        "        # TODO: keep score of your training loss and any other metric you see fit for\n",
        "        # your regression task\n",
        "        train_losses.append(loss.item())\n",
        "\n",
        "    train_loss_mean = np.mean(train_losses)\n",
        "    train_loss_history.append(train_loss_mean)\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_losses = []\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in val_loader:\n",
        "            # TODO: Same as training but we are not updating weights\n",
        "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "            outputs = model(X_batch)\n",
        "            val_loss = criterion(outputs, y_batch)\n",
        "            val_losses.append(val_loss.item())\n",
        "\n",
        "    # TODO: implementing early stopping here.\n",
        "    # keep track of the validation loss and save the model parameters\n",
        "    # when the loss is the lowest for the validation set\n",
        "    val_loss_mean = np.mean(val_losses)\n",
        "    val_loss_history.append(val_loss_mean)\n",
        "    if val_loss_mean < best_val_loss:\n",
        "        best_val_loss = val_loss_mean\n",
        "        best_epoch = epoch\n",
        "        torch.save(model.state_dict(), \"best_model.pt\")\n",
        "        no_improvement = 0\n",
        "    else:\n",
        "      no_improvement = no_improvement + 1\n",
        "      if no_improvement >= 10:\n",
        "        print(f\"\\nEarly stopping triggered after {best_epoch+1} epochs\")\n",
        "        break\n",
        "\n",
        "\n",
        "    # TODO: take a step in the scheduler to update the learning rate\n",
        "    scheduler.step()\n",
        "\n",
        "    # Print summary\n",
        "    print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {np.mean(train_losses):.4f} | Val Loss: {val_loss_mean:.4f} | LR: {scheduler.get_last_lr()[0]:.6f}\")\n",
        "\n",
        "print(f\"\\nBest model saved from epoch {best_epoch+1} with Val Loss: {best_val_loss:.4f}\")\n",
        "\n",
        "# ---------- PLOT TRAINING LOSS -------------\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(train_loss_history, '.-',label='Training Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Plot validation loss\n",
        "plt.plot(val_loss_history, 'g.-',label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vdYN3jijZg5E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------- TEST SET RESULTS ON OPTIMAL MODEL ----------------\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import mean_absolute_error, r2_score\n",
        "\n",
        "# TODO: we are to check the model's performance on the validation set\n",
        "# load the \"best_model.pt\" and pass your test data to it and get predictions\n",
        "# make sure the model is in eval mode, send the data to device\n",
        "# note: make sure you handled \"shuffle\" properly in your validation/testing set\n",
        "\n",
        "# loading the best model\n",
        "model.load_state_dict(torch.load(\"best_model.pt\", weights_only=True))\n",
        "model.eval()\n",
        "\n",
        "test_preds = []\n",
        "test_targets = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for X_batch, y_batch in test_loader:\n",
        "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "            preds = model(X_batch).squeeze()\n",
        "            test_preds.append(preds.cpu())\n",
        "            test_targets.append(y_batch.cpu())\n",
        "\n",
        "# concatenate everything\n",
        "y_pred = torch.cat(test_preds).numpy()\n",
        "y_true = torch.cat(test_targets).numpy()\n",
        "\n",
        "# Unscale predictions and targets before evaluation\n",
        "y_pred = scaler_y.inverse_transform(y_pred.reshape(-1, 1)).flatten()\n",
        "y_true = scaler_y.inverse_transform(y_true.reshape(-1, 1)).flatten()\n",
        "\n",
        "\n",
        "#TODO: plot regression plot and residual plots for your regression task,\n",
        "# report R2, and MAE along with any other metrics you want\n",
        "# analyze your results and report on your findings.\n",
        "\n",
        "\n",
        "# Calculate metrics\n",
        "mae = mean_absolute_error(y_true, y_pred)\n",
        "r2 = r2_score(y_true, y_pred)\n",
        "residuals = y_true - y_pred\n",
        "std_residuals = np.std(residuals)\n",
        "\n",
        "# Plot results on test data\n",
        "fig,ax = plt.subplots(1,2,figsize=(15,4))\n",
        "# Regression Plot\n",
        "ax[0].scatter(y_true, y_pred, alpha=0.5,edgecolors='black')\n",
        "ax[0].plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'r--', lw=2)\n",
        "ax[0].set_xlabel('Actual Chlorophyll Content')\n",
        "ax[0].set_ylabel('Predicted Chlorophyll Content')\n",
        "ax[0].set_title('Regression Plot')\n",
        "textstr = f\"MAE: {mae:.2f}\\nRÂ²: {r2:.2f}\"\n",
        "ax[0].text(0.05, 0.95, textstr,\n",
        "           transform=ax[0].transAxes,\n",
        "           verticalalignment='top',\n",
        "           horizontalalignment='left',\n",
        "           fontsize=10)\n",
        "\n",
        "# Residuals Plot\n",
        "ax[1].scatter(y_pred, residuals, color='g',alpha=0.5,edgecolors='black')\n",
        "ax[1].axhline(y=0, color='r', linestyle='--')\n",
        "ax[1].set_xlabel(\"Predicted\")\n",
        "ax[1].set_ylabel(\"Residuals\")\n",
        "ax[1].set_title(\"Residual Plot\")\n",
        "textstr = f\"Std Residuals: {std_residuals:.2f}\"\n",
        "ax[1].text(0.05, 0.95, textstr,\n",
        "           transform=ax[1].transAxes,\n",
        "           verticalalignment='top',\n",
        "           horizontalalignment='left',\n",
        "           fontsize=10)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.suptitle('Testing Data',fontsize=12,fontweight='bold')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "print(\"MAE:\", mean_absolute_error(y_true, y_pred))\n",
        "print(\"R2 Score:\", r2_score(y_true, y_pred))\n",
        "print(\"Std Residuals:\", np.std(residuals))\n"
      ],
      "metadata": {
        "id": "pVJ2c2MJaoey"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}