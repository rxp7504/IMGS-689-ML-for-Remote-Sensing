{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mOMVQGrPOdTf"
      },
      "outputs": [],
      "source": [
        "# Applications of Machine Learning in Remote Sensing\n",
        "# Homework 5 - Problem 2\n",
        "\n",
        "# import files and mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3CoAgvWZeg4W"
      },
      "source": [
        "# Problem 2.a: Using a Pretrained Model as a Feature Extractor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DWVKwIPSP2bC"
      },
      "outputs": [],
      "source": [
        "# Normalize all images using the standard ImageNet statistics\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# Convert images to tensors and normalize the mean and std to standard ImageNet stats\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # Resize images to 224x224 (required for ResNet)\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Assign the image folder and apply the transforms\n",
        "dataset = ImageFolder(root='/content/drive/MyDrive/Imaging Science MS/Applied ML for Remote Sensing/Homework/HW5 - Neural Networks/Problem 2/UCMerced_LandUse/Images',\n",
        "                      transform=transform)\n",
        "\n",
        "# Class labels per sample\n",
        "classes = np.array(dataset.targets)\n",
        "num_classes = len(np.unique(classes))\n",
        "print(f\"Number of classes: {num_classes}\")\n",
        "\n",
        "# Split into train and temp datasets (70% training / 15% validation / 15% testing)\n",
        "train_idx, temp_idx = train_test_split(\n",
        "    np.arange(len(classes)),\n",
        "    train_size=0.7,\n",
        "    stratify=classes,\n",
        "    random_state=15 # fix seed for deterministic split\n",
        ")\n",
        "\n",
        "# Split temp into validation and test sets\n",
        "val_idx, test_idx = train_test_split(\n",
        "    temp_idx,\n",
        "    test_size=0.5,\n",
        "    stratify=classes[temp_idx],\n",
        "    random_state=15 # fix seed for deterministic split\n",
        ")\n",
        "\n",
        "# Create the subsets for each division (instantiate datasets)\n",
        "train_dataset = Subset(dataset, train_idx)\n",
        "val_dataset   = Subset(dataset, val_idx)\n",
        "test_dataset  = Subset(dataset, test_idx)\n",
        "\n",
        "# Crate dataloaders for each division\n",
        "batch_size = 64\n",
        "num_workers = 2\n",
        "prefetch_factor = 3\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size,shuffle=True,num_workers=num_workers,pin_memory=True,prefetch_factor=prefetch_factor)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size,shuffle=True,num_workers=num_workers,pin_memory=True,prefetch_factor=prefetch_factor)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size,shuffle=True,num_workers=num_workers,pin_memory=True,prefetch_factor=prefetch_factor)\n",
        "\n",
        "#----------- Plot histograms to show class balance between datasets ------------\n",
        "\n",
        "# Extract class labels for each split\n",
        "train_labels = classes[train_idx]\n",
        "val_labels   = classes[val_idx]\n",
        "test_labels  = classes[test_idx]\n",
        "\n",
        "# Access class names\n",
        "class_names = dataset.classes\n",
        "\n",
        "fig,ax = plt.subplots(1,3,figsize=(15,5))\n",
        "# Train histogram\n",
        "ax[0].bar(range(len(class_names)), np.bincount(train_labels), edgecolor='black')\n",
        "ax[0].set_title('Train')\n",
        "ax[0].set_xlabel('Class')\n",
        "ax[0].set_ylabel('Count')\n",
        "ax[0].set_xticks(range(len(class_names)))\n",
        "ax[0].set_xticklabels(class_names, rotation=90)\n",
        "\n",
        "# Validation histogram\n",
        "ax[1].bar(range(len(class_names)), np.bincount(val_labels), color='g', edgecolor='black')\n",
        "ax[1].set_title('Validation')\n",
        "ax[1].set_xlabel('Class')\n",
        "ax[1].set_ylabel('Count')\n",
        "ax[1].set_xticks(range(len(class_names)))\n",
        "ax[1].set_xticklabels(class_names, rotation=90)\n",
        "\n",
        "# Test histogram\n",
        "ax[2].bar(range(len(class_names)), np.bincount(test_labels), color='r', edgecolor='black')\n",
        "ax[2].set_title('Test')\n",
        "ax[2].set_xlabel('Class')\n",
        "ax[2].set_ylabel('Count')\n",
        "ax[2].set_xticks(range(len(class_names)))\n",
        "ax[2].set_xticklabels(class_names, rotation=90)\n",
        "\n",
        "plt.suptitle('UCMerced Class Balance', fontsize=16)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4wt0088ocwqR"
      },
      "outputs": [],
      "source": [
        "# Using a Pretrained Model for Feature Extraction\n",
        "import torchvision.models as models\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "\n",
        "# Import a ResNet18 model with ImageNet pretrained weights\n",
        "resnet18 = models.resnet18(pretrained=True)\n",
        "\n",
        "# Remove the final classification head of the model\n",
        "model = nn.Sequential(*list(resnet18.children())[:-1]) # assign new model all layers except the last\n",
        "\n",
        "# Change runtime type to GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "print(\"Using:\", device)\n",
        "model.eval()\n",
        "\n",
        "def resnet_feature_extract(dataloader):\n",
        "\n",
        "  all_features = []\n",
        "  all_labels = []\n",
        "\n",
        "  with torch.no_grad():  # Don't track gradients\n",
        "    for X_batch, y_batch in dataloader:\n",
        "\n",
        "        # Send input images and classes to device\n",
        "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "\n",
        "        # Get the feature vector for the batch\n",
        "        features = model(X_batch)  # Forward pass\n",
        "\n",
        "        # Flatten the features\n",
        "        features = features.view(features.size(0), -1)  # Flatten to [batch_size, 512]\n",
        "\n",
        "        # Store the features and labels\n",
        "        all_features.append(features.cpu())  # Move to CPU for easier handling\n",
        "        all_labels.append(y_batch.cpu())  # Move to CPU for easier handling\n",
        "\n",
        "  # Concatenate\n",
        "  all_features = torch.cat(all_features, dim=0)\n",
        "  all_labels = torch.cat(all_labels, dim=0)\n",
        "  return all_features, all_labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VD5ogdklG64u"
      },
      "outputs": [],
      "source": [
        "#------------- Extract and save TRAINING set features -------------------\n",
        "train_features, train_labels = resnet_feature_extract(train_loader)\n",
        "\n",
        "# Print the sizes\n",
        "print(\"Training features shape:\", train_features.shape)\n",
        "print(\"Training labels shape:\", train_labels.shape)\n",
        "\n",
        "# Save the feature tensors\n",
        "torch.save(train_features, 'train_features.pt')\n",
        "torch.save(train_labels, 'train_labels.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h85-XQUKJzll"
      },
      "outputs": [],
      "source": [
        "#------------- Extract and save VALIDATION set features -------------------\n",
        "val_features, val_labels = resnet_feature_extract(val_loader)\n",
        "\n",
        "# Print the sizes\n",
        "print(\"Validation features shape:\", val_features.shape)\n",
        "print(\"Validation labels shape:\", val_labels.shape)\n",
        "\n",
        "# Save the feature tensors\n",
        "torch.save(val_features, 'val_features.pt')\n",
        "torch.save(val_labels, 'val_labels.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q63c6RPRKAcC"
      },
      "outputs": [],
      "source": [
        "#------------- Extract and save TEST set features -------------------\n",
        "test_features, test_labels = resnet_feature_extract(test_loader)\n",
        "\n",
        "# Print the sizes\n",
        "print(\"Testing features shape:\", test_features.shape)\n",
        "print(\"Testing labels shape:\", test_labels.shape)\n",
        "\n",
        "# Save the feature tensors\n",
        "torch.save(test_features, 'test_features.pt')\n",
        "torch.save(test_labels, 'test_labels.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ngAnQg4rJllH"
      },
      "outputs": [],
      "source": [
        "#---------- Load all saved feature tensors and labels for each division --------\n",
        "import numpy as np\n",
        "folderPath = '/content/drive/MyDrive/Imaging Science MS/Applied ML for Remote Sensing/Homework/HW5 - Neural Networks/Problem 2/Feature Data/'\n",
        "\n",
        "# Load the training feature tensors\n",
        "train_features = torch.load(folderPath + 'train_features.pt')\n",
        "train_labels = torch.load(folderPath + 'train_labels.pt')\n",
        "# Load the validation feature tensors\n",
        "val_features = torch.load(folderPath + 'val_features.pt')\n",
        "val_labels = torch.load(folderPath + 'val_labels.pt')\n",
        "\n",
        "# Load testing feature tensors\n",
        "test_features = torch.load(folderPath + 'test_features.pt')\n",
        "test_labels = torch.load(folderPath + 'test_labels.pt')\n",
        "\n",
        "# Number of classes\n",
        "num_classes = len(np.unique(test_labels))\n",
        "print(f\"Number of classes: {num_classes}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1AOpd6FgPC0F"
      },
      "outputs": [],
      "source": [
        "#--------------- Train an XGBoost classifier using the features ----------------\n",
        "import xgboost as xgb\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "\n",
        "# Convert to numpy arrays\n",
        "X_train = train_features.numpy()\n",
        "y_train = train_labels.numpy()\n",
        "\n",
        "X_val = val_features.numpy()\n",
        "y_val = val_labels.numpy()\n",
        "\n",
        "X_test = test_features.numpy()\n",
        "y_test = test_labels.numpy()\n",
        "\n",
        "# Standardize input data using StandardScaler\n",
        "scaler = StandardScaler() # initialize the scaler\n",
        "scaler.fit(X_train) # fit to only the training data\n",
        "X_train = scaler.transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# DMatrix format for xgboost\n",
        "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
        "dval = xgb.DMatrix(X_val, label=y_val)\n",
        "dtest = xgb.DMatrix(X_test, label=y_test)\n",
        "\n",
        "# Model hyperparameters\n",
        "params = {\n",
        "    \"objective\": \"multi:softmax\",  # Multi-class classification\n",
        "    \"num_class\": num_classes,\n",
        "    \"eta\": 0.1,  # Learning rate\n",
        "    \"seed\": 15  # Reproducibility\n",
        "}\n",
        "num_rounds = 100  # Number of boosting rounds\n",
        "\n",
        "# Train the model\n",
        "evals = [(dtrain, 'train'), (dval, 'eval')]\n",
        "model = xgb.train(params, dtrain, num_rounds,evals=evals,early_stopping_rounds=10)\n",
        "\n",
        "# Make predictions on TEST data\n",
        "y_pred = model.predict(dtest).astype(int)  # Returns class labels\n",
        "\n",
        "# Model performance metrics (use macro because classes are perfectly balanced)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred,average='macro')\n",
        "recall = recall_score(y_test, y_pred,average='macro')\n",
        "f1 = f1_score(y_test, y_pred,average='macro')\n",
        "\n",
        "print(f\"\\nAccuracy: {accuracy}\")\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(f\"F1 Score: {f1}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qQ7zaFIJXsle"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "# Plot the confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "fig, ax = plt.subplots(figsize=(10, 8))\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "disp.plot(ax=ax,cmap=\"Blues\")\n",
        "plt.title(\"XGBoost Confusion Matrix\")\n",
        "plt.show()\n",
        "\n",
        "# Plot the normalized confusion matrix\n",
        "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "cm_normalized = np.round(cm_normalized, 1)  # Round the normalized values to 1 decimal place\n",
        "fig, ax = plt.subplots(figsize=(10, 8))\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm_normalized)\n",
        "disp.plot(ax=ax,cmap=\"Blues\")\n",
        "plt.title(\"XGBoost Confusion Matrix (Normalized)\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9r5sJNxEeqTp"
      },
      "source": [
        "# Problem 2.b: Fine-tuning a Pretrained Model – Trainng Only the Head"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0obKBEYnXt18"
      },
      "outputs": [],
      "source": [
        "# Modify the Network\n",
        "import torchvision.models as models\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "from sklearn.metrics import accuracy_score\n",
        "# Import a ResNet18 model with ImageNet pretrained weights\n",
        "model = models.resnet18(pretrained=True)\n",
        "\n",
        "# Step 1. Grab the number of input features to the classification head\n",
        "num_ftrs = model.fc.in_features\n",
        "print(\"Number of input features:\", num_ftrs)\n",
        "\n",
        "# Replace the original head with a new one suited to your dataset\n",
        "model.fc = nn.Linear(num_ftrs,num_classes)\n",
        "\n",
        "\n",
        "# Step 2. Freeze the Base Model\n",
        "\n",
        "# Freeze all layers\n",
        "for param in model.parameters():\n",
        "  param.requires_grad = False\n",
        "\n",
        "# Unfreeze only the classification head\n",
        "for param in model.fc.parameters():\n",
        "  param.requires_grad = True\n",
        "\n",
        "# Change runtime type to GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "print(\"Using:\", device)\n",
        "\n",
        "# Step 3: Training Setup\n",
        "lr = 1e-3\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
        "\n",
        "# Initialize variables for early stopping\n",
        "best_val_loss = float(\"inf\")\n",
        "best_epoch = -1\n",
        "\n",
        "# keeping track of loss\n",
        "val_loss_history = []\n",
        "train_loss_history = []\n",
        "\n",
        "# keeping track of accuracy\n",
        "train_accuracy_history = []\n",
        "val_accuracy_history = []\n",
        "\n",
        "# For early stopping\n",
        "no_improvement = 0\n",
        "\n",
        "#-------------------------- Training loop -------------------------------------\n",
        "epochs = 50\n",
        "for epoch in range(epochs):\n",
        "    model.train() # set model to training mode\n",
        "    train_losses = [] # reset training losses\n",
        "    train_accuracies = [] # reset training accuracies\n",
        "\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        # send input features and reference target to device\n",
        "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "\n",
        "        # zeroing out previous step gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # make predictions using the model\n",
        "        outputs = model(X_batch)\n",
        "\n",
        "        # calculate the loss\n",
        "        loss = criterion(outputs, y_batch)\n",
        "\n",
        "        # calculate the gradients by calling backward on the loss\n",
        "        loss.backward()\n",
        "\n",
        "        # take a step by calling step on the optimizer\n",
        "        optimizer.step()\n",
        "\n",
        "        # keep score of training loss\n",
        "        train_losses.append(loss.item())\n",
        "\n",
        "        # calculate accuracy\n",
        "        preds = torch.argmax(outputs, dim=1)\n",
        "        train_accuracy = accuracy_score(y_batch.cpu().numpy(), preds.detach().cpu().numpy())\n",
        "\n",
        "        # keep score of training accuracy\n",
        "        train_accuracies.append(train_accuracy)\n",
        "\n",
        "\n",
        "    train_loss_mean = np.mean(train_losses) # average train loss for current epoch\n",
        "    train_loss_history.append(train_loss_mean) # add to loss per epoch list\n",
        "\n",
        "    train_accuracy_mean = np.mean(train_accuracies) # average train accuracy for current epoch\n",
        "    train_accuracy_history.append(train_accuracy_mean) # add to accuracy per epoch list\n",
        "\n",
        "\n",
        "    #------------------------ Validation Loop ----------------------------------\n",
        "    model.eval() # set model to evaluation mode\n",
        "    val_losses = [] # reset val losses\n",
        "    val_accuracies = [] # reset val accuracies\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in val_loader:\n",
        "            # Same as training but we are not updating weights\n",
        "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "            outputs = model(X_batch)\n",
        "\n",
        "            # validation loss\n",
        "            val_loss = criterion(outputs, y_batch)\n",
        "            val_losses.append(val_loss.item())\n",
        "\n",
        "            # validation accuracy\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "            val_accuracy = accuracy_score(y_batch.cpu().numpy(), preds.detach().cpu().numpy())\n",
        "            val_accuracies.append(val_accuracy) # keep score of val accuracy\n",
        "\n",
        "    # Implementing early stopping here.\n",
        "    # Keep track of the validation loss and save the model parameters\n",
        "    # when the loss is the lowest for the validation set\n",
        "    val_loss_mean = np.mean(val_losses) # average val loss for current epoch\n",
        "    val_loss_history.append(val_loss_mean) # add to loss per epoch list\n",
        "    val_accuracy_mean = np.mean(val_accuracies) # average val accuracy for current epoch\n",
        "    val_accuracy_history.append(val_accuracy_mean) # add to accuracy per epoch list\n",
        "    if val_loss_mean < best_val_loss:\n",
        "        best_val_loss = val_loss_mean # update the new best val loss\n",
        "        best_epoch = epoch # update the best epoch\n",
        "        torch.save(model.state_dict(), \"best_model.pt\") # save the best model params\n",
        "        no_improvement = 0 # reset time since improvement\n",
        "    else:\n",
        "      no_improvement = no_improvement + 1\n",
        "      if no_improvement >= 5: # early stop if no improvement for 5 epochs\n",
        "        print(f\"\\nEarly stopping triggered after {best_epoch+1} epochs\")\n",
        "        break\n",
        "\n",
        "\n",
        "    # take a step in the scheduler to update the learning rate\n",
        "    scheduler.step()\n",
        "\n",
        "    # Print summary\n",
        "    print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {np.mean(train_losses):.4f} | Val Loss: {val_loss_mean:.4f} | LR: {scheduler.get_last_lr()[0]:.6f}\")\n",
        "\n",
        "print(f\"\\nBest model saved from epoch {best_epoch+1} with Val Loss: {best_val_loss:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4aN-lWgoVS_U"
      },
      "outputs": [],
      "source": [
        "# Plot both training and validation losses across epochs\n",
        "# plt.figure(figsize=(12, 4))\n",
        "plt.plot(train_loss_history, label='Training Loss')\n",
        "plt.plot(val_loss_history, label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Plot both training and validation accuracy across epochs\n",
        "# plt.figure(figsize=(12, 4))\n",
        "plt.plot(train_accuracy_history, label='Training Accuracy')\n",
        "plt.plot(val_accuracy_history, label='Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-avFbpSCeZtc"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "# Step 4: Evaluation\n",
        "\n",
        "# --------------- Evaluate Model Performance on the Test Set -------------------\n",
        "\n",
        "# Load the best model\n",
        "model.load_state_dict(torch.load(\"best_model.pt\", weights_only=True))\n",
        "model.eval()\n",
        "\n",
        "all_preds = []\n",
        "all_targets = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for X_batch, y_batch in test_loader:\n",
        "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "        outputs = model(X_batch)\n",
        "        preds = torch.argmax(outputs, dim=1)\n",
        "        all_preds.append(preds.cpu())\n",
        "        all_targets.append(y_batch.cpu())\n",
        "\n",
        "# Concatenate all batches\n",
        "all_preds = torch.cat(all_preds).numpy()\n",
        "all_targets = torch.cat(all_targets).numpy()\n",
        "\n",
        "# Now compute metrics once\n",
        "test_accuracy = accuracy_score(all_targets, all_preds)\n",
        "test_precision = precision_score(all_targets, all_preds, average='macro')\n",
        "test_recall = recall_score(all_targets, all_preds, average='macro')\n",
        "test_f1 = f1_score(all_targets, all_preds, average='macro')\n",
        "\n",
        "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
        "print(f\"Test Precision: {test_precision:.4f}\")\n",
        "print(f\"Test Recall: {test_recall:.4f}\")\n",
        "print(f\"Test F1 Score: {test_f1:.4f}\")\n",
        "\n",
        "# ------------------ Plot Confusion Matrix for Test Data -----------------------\n",
        "\n",
        "# Plot the confusion matrix\n",
        "cm = confusion_matrix(all_targets, all_preds)\n",
        "fig, ax = plt.subplots(figsize=(10, 8))\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "disp.plot(ax=ax,cmap=\"Blues\")\n",
        "plt.title(\"Confusion Matrix (Training Only Head)\")\n",
        "plt.show()\n",
        "\n",
        "# Plot the normalized confusion matrix\n",
        "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "cm_normalized = np.round(cm_normalized, 1)  # Round the normalized values to 1 decimal place\n",
        "fig, ax = plt.subplots(figsize=(10, 8))\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm_normalized)\n",
        "disp.plot(ax=ax,cmap=\"Blues\")\n",
        "plt.title(\"Confusion Matrix (Training Only Head - Normalized)\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNsZ-p6Xjdqr"
      },
      "source": [
        "# Problem 2.c: Fine-tuning a Pretrained Model – Retraining the Entire Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iaA3GYIMjYYv"
      },
      "outputs": [],
      "source": [
        "# Step 1: Modify and Unfreeze the Network\n",
        "\n",
        "# Import a ResNet18 model with ImageNet pretrained weights\n",
        "model = models.resnet18(pretrained=True)\n",
        "\n",
        "# Replace final classification head\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Linear(num_ftrs,num_classes)\n",
        "\n",
        "# Make all layers trainable\n",
        "for param in model.parameters():\n",
        "  param.requires_grad = True\n",
        "\n",
        "# Change runtime type to GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "print(\"Using:\", device)\n",
        "\n",
        "# Step 2: Training Setup\n",
        "lr = 1e-4\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
        "\n",
        "# Initialize variables for early stopping\n",
        "best_val_loss = float(\"inf\")\n",
        "best_epoch = -1\n",
        "\n",
        "# keeping track of loss\n",
        "val_loss_history = []\n",
        "train_loss_history = []\n",
        "\n",
        "# keeping track of accuracy\n",
        "train_accuracy_history = []\n",
        "val_accuracy_history = []\n",
        "\n",
        "# For early stopping\n",
        "no_improvement = 0\n",
        "\n",
        "\n",
        "#-------------------------- Training loop -------------------------------------\n",
        "epochs = 50\n",
        "for epoch in range(epochs):\n",
        "    model.train() # set model to training mode\n",
        "    train_losses = [] # reset training losses\n",
        "    train_accuracies = [] # reset training accuracies\n",
        "\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        # send input features and reference target to device\n",
        "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "\n",
        "        # zeroing out previous step gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # make predictions using the model\n",
        "        outputs = model(X_batch)\n",
        "\n",
        "        # calculate the loss\n",
        "        loss = criterion(outputs, y_batch)\n",
        "\n",
        "        # calculate the gradients by calling backward on the loss\n",
        "        loss.backward()\n",
        "\n",
        "        # take a step by calling step on the optimizer\n",
        "        optimizer.step()\n",
        "\n",
        "        # keep score of training loss\n",
        "        train_losses.append(loss.item())\n",
        "\n",
        "        # calculate accuracy\n",
        "        preds = torch.argmax(outputs, dim=1)\n",
        "        train_accuracy = accuracy_score(y_batch.cpu().numpy(), preds.detach().cpu().numpy())\n",
        "\n",
        "        # keep score of training accuracy\n",
        "        train_accuracies.append(train_accuracy)\n",
        "\n",
        "\n",
        "    train_loss_mean = np.mean(train_losses) # average train loss for current epoch\n",
        "    train_loss_history.append(train_loss_mean) # add to loss per epoch list\n",
        "\n",
        "    train_accuracy_mean = np.mean(train_accuracies) # average train accuracy for current epoch\n",
        "    train_accuracy_history.append(train_accuracy_mean) # add to accuracy per epoch list\n",
        "\n",
        "\n",
        "    #------------------------ Validation Loop ----------------------------------\n",
        "    model.eval() # set model to evaluation mode\n",
        "    val_losses = [] # reset val losses\n",
        "    val_accuracies = [] # reset val accuracies\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in val_loader:\n",
        "            # Same as training but we are not updating weights\n",
        "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "            outputs = model(X_batch)\n",
        "\n",
        "            # validation loss\n",
        "            val_loss = criterion(outputs, y_batch)\n",
        "            val_losses.append(val_loss.item())\n",
        "\n",
        "            # validation accuracy\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "            val_accuracy = accuracy_score(y_batch.cpu().numpy(), preds.detach().cpu().numpy())\n",
        "            val_accuracies.append(val_accuracy) # keep score of val accuracy\n",
        "\n",
        "    # Implementing early stopping here.\n",
        "    # Keep track of the validation loss and save the model parameters\n",
        "    # when the loss is the lowest for the validation set\n",
        "    val_loss_mean = np.mean(val_losses) # average val loss for current epoch\n",
        "    val_loss_history.append(val_loss_mean) # add to loss per epoch list\n",
        "    val_accuracy_mean = np.mean(val_accuracies) # average val accuracy for current epoch\n",
        "    val_accuracy_history.append(val_accuracy_mean) # add to accuracy per epoch list\n",
        "    if val_loss_mean < best_val_loss:\n",
        "        best_val_loss = val_loss_mean # update the new best val loss\n",
        "        best_epoch = epoch # update the best epoch\n",
        "        torch.save(model.state_dict(), \"best_model.pt\") # save the best model params\n",
        "        no_improvement = 0 # reset time since improvement\n",
        "    else:\n",
        "      no_improvement = no_improvement + 1\n",
        "      if no_improvement >= 5: # early stop if no improvement for 5 epochs\n",
        "        print(f\"\\nEarly stopping triggered after {best_epoch+1} epochs\")\n",
        "        break\n",
        "\n",
        "\n",
        "    # take a step in the scheduler to update the learning rate\n",
        "    scheduler.step()\n",
        "\n",
        "    # Print summary\n",
        "    print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {np.mean(train_losses):.4f} | Val Loss: {val_loss_mean:.4f} | LR: {scheduler.get_last_lr()[0]:.6f}\")\n",
        "\n",
        "print(f\"\\nBest model saved from epoch {best_epoch+1} with Val Loss: {best_val_loss:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ww6pRw1imjqE"
      },
      "outputs": [],
      "source": [
        "# Plot both training and validation losses across epochs\n",
        "plt.plot(train_loss_history, label='Training Loss')\n",
        "plt.plot(val_loss_history, label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Plot both training and validation accuracy across epochs\n",
        "plt.plot(train_accuracy_history, label='Training Accuracy')\n",
        "plt.plot(val_accuracy_history, label='Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vm2lO7ATmrTC"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "# Step 4: Evaluation\n",
        "\n",
        "# --------------- Evaluate Model Performance on the Test Set -------------------\n",
        "\n",
        "# Load the best model\n",
        "model.load_state_dict(torch.load(\"best_model.pt\", weights_only=True))\n",
        "model.eval()\n",
        "\n",
        "all_preds = []\n",
        "all_targets = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for X_batch, y_batch in test_loader:\n",
        "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "        outputs = model(X_batch)\n",
        "        preds = torch.argmax(outputs, dim=1)\n",
        "        all_preds.append(preds.cpu())\n",
        "        all_targets.append(y_batch.cpu())\n",
        "\n",
        "# Concatenate all batches\n",
        "all_preds = torch.cat(all_preds).numpy()\n",
        "all_targets = torch.cat(all_targets).numpy()\n",
        "\n",
        "# Now compute metrics once\n",
        "test_accuracy = accuracy_score(all_targets, all_preds)\n",
        "test_precision = precision_score(all_targets, all_preds, average='macro')\n",
        "test_recall = recall_score(all_targets, all_preds, average='macro')\n",
        "test_f1 = f1_score(all_targets, all_preds, average='macro')\n",
        "\n",
        "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
        "print(f\"Test Precision: {test_precision:.4f}\")\n",
        "print(f\"Test Recall: {test_recall:.4f}\")\n",
        "print(f\"Test F1 Score: {test_f1:.4f}\")\n",
        "\n",
        "# ------------------ Plot Confusion Matrix for Test Data -----------------------\n",
        "\n",
        "# Plot the confusion matrix\n",
        "cm = confusion_matrix(all_targets, all_preds)\n",
        "fig, ax = plt.subplots(figsize=(10, 8))\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "disp.plot(ax=ax,cmap=\"Blues\")\n",
        "plt.title(\"Confusion Matrix (Full Model Training)\")\n",
        "plt.show()\n",
        "\n",
        "# Plot the normalized confusion matrix\n",
        "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "cm_normalized = np.round(cm_normalized, 1)  # Round the normalized values to 1 decimal place\n",
        "fig, ax = plt.subplots(figsize=(10, 8))\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm_normalized)\n",
        "disp.plot(ax=ax,cmap=\"Blues\")\n",
        "plt.title(\"Confusion Matrix (Full Model Training - Normalized)\")\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}